{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 6480,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023148148148148147,
      "grad_norm": 45.305091857910156,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 5.6025,
      "step": 10
    },
    {
      "epoch": 0.046296296296296294,
      "grad_norm": 64.61473846435547,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 4.7302,
      "step": 20
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 84.87616729736328,
      "learning_rate": 3e-06,
      "loss": 4.4604,
      "step": 30
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 60.95634078979492,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.6331,
      "step": 40
    },
    {
      "epoch": 0.11574074074074074,
      "grad_norm": 10.367708206176758,
      "learning_rate": 5e-06,
      "loss": 1.9902,
      "step": 50
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 5.929224491119385,
      "learning_rate": 6e-06,
      "loss": 1.5442,
      "step": 60
    },
    {
      "epoch": 0.16203703703703703,
      "grad_norm": 5.229705810546875,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.9732,
      "step": 70
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 5.081185817718506,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.5194,
      "step": 80
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 5.539037227630615,
      "learning_rate": 9e-06,
      "loss": 1.5064,
      "step": 90
    },
    {
      "epoch": 0.23148148148148148,
      "grad_norm": 5.79301118850708,
      "learning_rate": 1e-05,
      "loss": 1.5908,
      "step": 100
    },
    {
      "epoch": 0.25462962962962965,
      "grad_norm": 3.2404580116271973,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.3886,
      "step": 110
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 4.523477077484131,
      "learning_rate": 1.2e-05,
      "loss": 1.3677,
      "step": 120
    },
    {
      "epoch": 0.30092592592592593,
      "grad_norm": 5.525075912475586,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.5318,
      "step": 130
    },
    {
      "epoch": 0.32407407407407407,
      "grad_norm": 7.066387176513672,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.472,
      "step": 140
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 4.458609104156494,
      "learning_rate": 1.5e-05,
      "loss": 1.0948,
      "step": 150
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 4.8154296875,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.3013,
      "step": 160
    },
    {
      "epoch": 0.39351851851851855,
      "grad_norm": 5.198561668395996,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.4659,
      "step": 170
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 4.9944329261779785,
      "learning_rate": 1.8e-05,
      "loss": 1.1424,
      "step": 180
    },
    {
      "epoch": 0.4398148148148148,
      "grad_norm": 5.291311740875244,
      "learning_rate": 1.9e-05,
      "loss": 1.1296,
      "step": 190
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 5.868687152862549,
      "learning_rate": 2e-05,
      "loss": 1.2671,
      "step": 200
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 5.833006858825684,
      "learning_rate": 2.1e-05,
      "loss": 1.209,
      "step": 210
    },
    {
      "epoch": 0.5092592592592593,
      "grad_norm": 6.480470657348633,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.2172,
      "step": 220
    },
    {
      "epoch": 0.5324074074074074,
      "grad_norm": 4.7796173095703125,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.1552,
      "step": 230
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 3.9937894344329834,
      "learning_rate": 2.4e-05,
      "loss": 1.3495,
      "step": 240
    },
    {
      "epoch": 0.5787037037037037,
      "grad_norm": 4.9446587562561035,
      "learning_rate": 2.5e-05,
      "loss": 1.0696,
      "step": 250
    },
    {
      "epoch": 0.6018518518518519,
      "grad_norm": 5.388405799865723,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.2182,
      "step": 260
    },
    {
      "epoch": 0.625,
      "grad_norm": 5.3935418128967285,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.3611,
      "step": 270
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 4.178459167480469,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.3609,
      "step": 280
    },
    {
      "epoch": 0.6712962962962963,
      "grad_norm": 5.0972466468811035,
      "learning_rate": 2.9e-05,
      "loss": 1.2795,
      "step": 290
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 6.244102478027344,
      "learning_rate": 3e-05,
      "loss": 1.4594,
      "step": 300
    },
    {
      "epoch": 0.7175925925925926,
      "grad_norm": 5.38688325881958,
      "learning_rate": 3.1e-05,
      "loss": 1.0227,
      "step": 310
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 4.075433731079102,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.129,
      "step": 320
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 4.533299922943115,
      "learning_rate": 3.3e-05,
      "loss": 1.2291,
      "step": 330
    },
    {
      "epoch": 0.7870370370370371,
      "grad_norm": 4.67616605758667,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.2755,
      "step": 340
    },
    {
      "epoch": 0.8101851851851852,
      "grad_norm": 4.3323540687561035,
      "learning_rate": 3.5e-05,
      "loss": 1.2015,
      "step": 350
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 4.897003173828125,
      "learning_rate": 3.6e-05,
      "loss": 1.1447,
      "step": 360
    },
    {
      "epoch": 0.8564814814814815,
      "grad_norm": 5.9306535720825195,
      "learning_rate": 3.7e-05,
      "loss": 1.226,
      "step": 370
    },
    {
      "epoch": 0.8796296296296297,
      "grad_norm": 2.8895039558410645,
      "learning_rate": 3.8e-05,
      "loss": 0.7673,
      "step": 380
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 4.686637878417969,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.3157,
      "step": 390
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 5.280179977416992,
      "learning_rate": 4e-05,
      "loss": 1.2192,
      "step": 400
    },
    {
      "epoch": 0.9490740740740741,
      "grad_norm": 5.7871994972229,
      "learning_rate": 4.1e-05,
      "loss": 1.1198,
      "step": 410
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 6.9369683265686035,
      "learning_rate": 4.2e-05,
      "loss": 1.5132,
      "step": 420
    },
    {
      "epoch": 0.9953703703703703,
      "grad_norm": 3.3938026428222656,
      "learning_rate": 4.3e-05,
      "loss": 1.0256,
      "step": 430
    },
    {
      "epoch": 1.0185185185185186,
      "grad_norm": 5.020218372344971,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.0525,
      "step": 440
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 4.760457992553711,
      "learning_rate": 4.5e-05,
      "loss": 1.372,
      "step": 450
    },
    {
      "epoch": 1.0648148148148149,
      "grad_norm": 4.509151935577393,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.2722,
      "step": 460
    },
    {
      "epoch": 1.087962962962963,
      "grad_norm": 4.012835502624512,
      "learning_rate": 4.7e-05,
      "loss": 1.2423,
      "step": 470
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 3.456350803375244,
      "learning_rate": 4.8e-05,
      "loss": 0.9213,
      "step": 480
    },
    {
      "epoch": 1.1342592592592593,
      "grad_norm": 3.8492653369903564,
      "learning_rate": 4.9e-05,
      "loss": 1.3575,
      "step": 490
    },
    {
      "epoch": 1.1574074074074074,
      "grad_norm": 3.7377588748931885,
      "learning_rate": 5e-05,
      "loss": 1.125,
      "step": 500
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 4.834456443786621,
      "learning_rate": 4.9916387959866226e-05,
      "loss": 1.2882,
      "step": 510
    },
    {
      "epoch": 1.2037037037037037,
      "grad_norm": 4.055736064910889,
      "learning_rate": 4.983277591973244e-05,
      "loss": 1.1666,
      "step": 520
    },
    {
      "epoch": 1.2268518518518519,
      "grad_norm": 5.59295129776001,
      "learning_rate": 4.974916387959867e-05,
      "loss": 1.2465,
      "step": 530
    },
    {
      "epoch": 1.25,
      "grad_norm": 4.805997371673584,
      "learning_rate": 4.9665551839464884e-05,
      "loss": 0.9079,
      "step": 540
    },
    {
      "epoch": 1.2731481481481481,
      "grad_norm": 4.966823577880859,
      "learning_rate": 4.958193979933111e-05,
      "loss": 1.2069,
      "step": 550
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 3.859898805618286,
      "learning_rate": 4.9498327759197325e-05,
      "loss": 1.2084,
      "step": 560
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 3.9114325046539307,
      "learning_rate": 4.941471571906355e-05,
      "loss": 1.0567,
      "step": 570
    },
    {
      "epoch": 1.3425925925925926,
      "grad_norm": 5.139212608337402,
      "learning_rate": 4.9331103678929766e-05,
      "loss": 1.3655,
      "step": 580
    },
    {
      "epoch": 1.3657407407407407,
      "grad_norm": 5.656362056732178,
      "learning_rate": 4.924749163879599e-05,
      "loss": 1.1173,
      "step": 590
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 3.5928168296813965,
      "learning_rate": 4.916387959866221e-05,
      "loss": 1.275,
      "step": 600
    },
    {
      "epoch": 1.412037037037037,
      "grad_norm": 4.977569103240967,
      "learning_rate": 4.908026755852843e-05,
      "loss": 1.0402,
      "step": 610
    },
    {
      "epoch": 1.4351851851851851,
      "grad_norm": 3.688474655151367,
      "learning_rate": 4.8996655518394655e-05,
      "loss": 1.1938,
      "step": 620
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 2.9693589210510254,
      "learning_rate": 4.891304347826087e-05,
      "loss": 1.0728,
      "step": 630
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 3.743941068649292,
      "learning_rate": 4.8829431438127096e-05,
      "loss": 1.0538,
      "step": 640
    },
    {
      "epoch": 1.5046296296296298,
      "grad_norm": 6.025543212890625,
      "learning_rate": 4.874581939799331e-05,
      "loss": 1.2836,
      "step": 650
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 3.222210168838501,
      "learning_rate": 4.866220735785954e-05,
      "loss": 1.0633,
      "step": 660
    },
    {
      "epoch": 1.550925925925926,
      "grad_norm": 4.207979679107666,
      "learning_rate": 4.8578595317725754e-05,
      "loss": 0.9342,
      "step": 670
    },
    {
      "epoch": 1.574074074074074,
      "grad_norm": 4.165318965911865,
      "learning_rate": 4.849498327759198e-05,
      "loss": 1.0387,
      "step": 680
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 4.999916076660156,
      "learning_rate": 4.8411371237458195e-05,
      "loss": 1.2009,
      "step": 690
    },
    {
      "epoch": 1.6203703703703702,
      "grad_norm": 4.083103179931641,
      "learning_rate": 4.832775919732442e-05,
      "loss": 0.9853,
      "step": 700
    },
    {
      "epoch": 1.6435185185185186,
      "grad_norm": 3.692363977432251,
      "learning_rate": 4.8244147157190636e-05,
      "loss": 0.9932,
      "step": 710
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 4.155097961425781,
      "learning_rate": 4.816053511705686e-05,
      "loss": 0.9201,
      "step": 720
    },
    {
      "epoch": 1.6898148148148149,
      "grad_norm": 4.747111797332764,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 0.9401,
      "step": 730
    },
    {
      "epoch": 1.7129629629629628,
      "grad_norm": 3.9082207679748535,
      "learning_rate": 4.79933110367893e-05,
      "loss": 1.2273,
      "step": 740
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 4.238386154174805,
      "learning_rate": 4.7909698996655525e-05,
      "loss": 1.1391,
      "step": 750
    },
    {
      "epoch": 1.7592592592592593,
      "grad_norm": 4.275578498840332,
      "learning_rate": 4.782608695652174e-05,
      "loss": 0.7668,
      "step": 760
    },
    {
      "epoch": 1.7824074074074074,
      "grad_norm": 3.2317373752593994,
      "learning_rate": 4.7742474916387966e-05,
      "loss": 1.0075,
      "step": 770
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 2.535519599914551,
      "learning_rate": 4.765886287625418e-05,
      "loss": 0.8652,
      "step": 780
    },
    {
      "epoch": 1.8287037037037037,
      "grad_norm": 4.416997909545898,
      "learning_rate": 4.7575250836120407e-05,
      "loss": 1.3143,
      "step": 790
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 3.6479721069335938,
      "learning_rate": 4.7491638795986624e-05,
      "loss": 1.0553,
      "step": 800
    },
    {
      "epoch": 1.875,
      "grad_norm": 3.5126466751098633,
      "learning_rate": 4.740802675585285e-05,
      "loss": 1.1871,
      "step": 810
    },
    {
      "epoch": 1.8981481481481481,
      "grad_norm": 4.919516563415527,
      "learning_rate": 4.7324414715719065e-05,
      "loss": 0.9464,
      "step": 820
    },
    {
      "epoch": 1.9212962962962963,
      "grad_norm": 3.110694646835327,
      "learning_rate": 4.724080267558529e-05,
      "loss": 1.0143,
      "step": 830
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 2.3799288272857666,
      "learning_rate": 4.715719063545151e-05,
      "loss": 1.0692,
      "step": 840
    },
    {
      "epoch": 1.9675925925925926,
      "grad_norm": 3.4754316806793213,
      "learning_rate": 4.707357859531773e-05,
      "loss": 1.3318,
      "step": 850
    },
    {
      "epoch": 1.9907407407407407,
      "grad_norm": 4.195021629333496,
      "learning_rate": 4.698996655518395e-05,
      "loss": 1.1318,
      "step": 860
    },
    {
      "epoch": 2.013888888888889,
      "grad_norm": 3.4872517585754395,
      "learning_rate": 4.690635451505017e-05,
      "loss": 0.9971,
      "step": 870
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 3.5786516666412354,
      "learning_rate": 4.6822742474916394e-05,
      "loss": 1.1218,
      "step": 880
    },
    {
      "epoch": 2.060185185185185,
      "grad_norm": 3.2964468002319336,
      "learning_rate": 4.673913043478261e-05,
      "loss": 1.1766,
      "step": 890
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 3.421034097671509,
      "learning_rate": 4.6655518394648835e-05,
      "loss": 1.1088,
      "step": 900
    },
    {
      "epoch": 2.1064814814814814,
      "grad_norm": 3.078023672103882,
      "learning_rate": 4.657190635451505e-05,
      "loss": 1.022,
      "step": 910
    },
    {
      "epoch": 2.1296296296296298,
      "grad_norm": 3.8436317443847656,
      "learning_rate": 4.6488294314381276e-05,
      "loss": 1.1233,
      "step": 920
    },
    {
      "epoch": 2.1527777777777777,
      "grad_norm": 3.295222520828247,
      "learning_rate": 4.640468227424749e-05,
      "loss": 0.9399,
      "step": 930
    },
    {
      "epoch": 2.175925925925926,
      "grad_norm": 3.746919870376587,
      "learning_rate": 4.632107023411372e-05,
      "loss": 0.9689,
      "step": 940
    },
    {
      "epoch": 2.199074074074074,
      "grad_norm": 4.319670677185059,
      "learning_rate": 4.623745819397994e-05,
      "loss": 1.0019,
      "step": 950
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 3.1287596225738525,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.9437,
      "step": 960
    },
    {
      "epoch": 2.2453703703703702,
      "grad_norm": 3.574404001235962,
      "learning_rate": 4.607023411371238e-05,
      "loss": 1.1078,
      "step": 970
    },
    {
      "epoch": 2.2685185185185186,
      "grad_norm": 4.177875518798828,
      "learning_rate": 4.59866220735786e-05,
      "loss": 1.1306,
      "step": 980
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 4.139176845550537,
      "learning_rate": 4.590301003344482e-05,
      "loss": 0.9803,
      "step": 990
    },
    {
      "epoch": 2.314814814814815,
      "grad_norm": 2.7819435596466064,
      "learning_rate": 4.581939799331103e-05,
      "loss": 0.9999,
      "step": 1000
    },
    {
      "epoch": 2.337962962962963,
      "grad_norm": 4.052305698394775,
      "learning_rate": 4.573578595317726e-05,
      "loss": 0.8899,
      "step": 1010
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 3.4402291774749756,
      "learning_rate": 4.565217391304348e-05,
      "loss": 0.9703,
      "step": 1020
    },
    {
      "epoch": 2.384259259259259,
      "grad_norm": 3.8832335472106934,
      "learning_rate": 4.55685618729097e-05,
      "loss": 1.0594,
      "step": 1030
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 2.766326904296875,
      "learning_rate": 4.548494983277592e-05,
      "loss": 0.9983,
      "step": 1040
    },
    {
      "epoch": 2.4305555555555554,
      "grad_norm": 3.093533515930176,
      "learning_rate": 4.540133779264214e-05,
      "loss": 1.1428,
      "step": 1050
    },
    {
      "epoch": 2.4537037037037037,
      "grad_norm": 3.5015878677368164,
      "learning_rate": 4.531772575250836e-05,
      "loss": 0.8469,
      "step": 1060
    },
    {
      "epoch": 2.476851851851852,
      "grad_norm": 3.0227208137512207,
      "learning_rate": 4.523411371237458e-05,
      "loss": 0.9077,
      "step": 1070
    },
    {
      "epoch": 2.5,
      "grad_norm": 4.691117286682129,
      "learning_rate": 4.5150501672240804e-05,
      "loss": 1.0032,
      "step": 1080
    },
    {
      "epoch": 2.523148148148148,
      "grad_norm": 2.9838240146636963,
      "learning_rate": 4.506688963210702e-05,
      "loss": 0.8762,
      "step": 1090
    },
    {
      "epoch": 2.5462962962962963,
      "grad_norm": 2.2940354347229004,
      "learning_rate": 4.4983277591973245e-05,
      "loss": 1.0796,
      "step": 1100
    },
    {
      "epoch": 2.5694444444444446,
      "grad_norm": 3.344007730484009,
      "learning_rate": 4.489966555183946e-05,
      "loss": 1.0054,
      "step": 1110
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 3.6487298011779785,
      "learning_rate": 4.4816053511705686e-05,
      "loss": 0.8238,
      "step": 1120
    },
    {
      "epoch": 2.6157407407407405,
      "grad_norm": 2.9600603580474854,
      "learning_rate": 4.473244147157191e-05,
      "loss": 1.0694,
      "step": 1130
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 3.1991193294525146,
      "learning_rate": 4.464882943143813e-05,
      "loss": 1.0145,
      "step": 1140
    },
    {
      "epoch": 2.662037037037037,
      "grad_norm": 3.669609546661377,
      "learning_rate": 4.456521739130435e-05,
      "loss": 0.9542,
      "step": 1150
    },
    {
      "epoch": 2.685185185185185,
      "grad_norm": 3.0882816314697266,
      "learning_rate": 4.448160535117057e-05,
      "loss": 0.8988,
      "step": 1160
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 2.652369737625122,
      "learning_rate": 4.439799331103679e-05,
      "loss": 1.1771,
      "step": 1170
    },
    {
      "epoch": 2.7314814814814814,
      "grad_norm": 3.318840980529785,
      "learning_rate": 4.431438127090301e-05,
      "loss": 0.925,
      "step": 1180
    },
    {
      "epoch": 2.7546296296296298,
      "grad_norm": 4.089377403259277,
      "learning_rate": 4.423076923076923e-05,
      "loss": 0.9582,
      "step": 1190
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 3.385077714920044,
      "learning_rate": 4.414715719063545e-05,
      "loss": 1.0394,
      "step": 1200
    },
    {
      "epoch": 2.800925925925926,
      "grad_norm": 4.0599541664123535,
      "learning_rate": 4.4063545150501674e-05,
      "loss": 1.0393,
      "step": 1210
    },
    {
      "epoch": 2.824074074074074,
      "grad_norm": 3.3871874809265137,
      "learning_rate": 4.397993311036789e-05,
      "loss": 0.8799,
      "step": 1220
    },
    {
      "epoch": 2.8472222222222223,
      "grad_norm": 3.9238383769989014,
      "learning_rate": 4.3896321070234115e-05,
      "loss": 0.899,
      "step": 1230
    },
    {
      "epoch": 2.8703703703703702,
      "grad_norm": 2.7311084270477295,
      "learning_rate": 4.381270903010034e-05,
      "loss": 1.0154,
      "step": 1240
    },
    {
      "epoch": 2.8935185185185186,
      "grad_norm": 4.536362648010254,
      "learning_rate": 4.3729096989966556e-05,
      "loss": 1.1315,
      "step": 1250
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 3.087096929550171,
      "learning_rate": 4.364548494983278e-05,
      "loss": 1.0818,
      "step": 1260
    },
    {
      "epoch": 2.939814814814815,
      "grad_norm": 2.8922629356384277,
      "learning_rate": 4.3561872909698996e-05,
      "loss": 1.1533,
      "step": 1270
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 2.8729236125946045,
      "learning_rate": 4.347826086956522e-05,
      "loss": 0.8725,
      "step": 1280
    },
    {
      "epoch": 2.986111111111111,
      "grad_norm": 3.1636266708374023,
      "learning_rate": 4.339464882943144e-05,
      "loss": 1.0196,
      "step": 1290
    },
    {
      "epoch": 3.009259259259259,
      "grad_norm": 2.4627532958984375,
      "learning_rate": 4.331103678929766e-05,
      "loss": 0.8715,
      "step": 1300
    },
    {
      "epoch": 3.0324074074074074,
      "grad_norm": 3.0958008766174316,
      "learning_rate": 4.322742474916388e-05,
      "loss": 0.8639,
      "step": 1310
    },
    {
      "epoch": 3.0555555555555554,
      "grad_norm": 3.098092555999756,
      "learning_rate": 4.31438127090301e-05,
      "loss": 0.939,
      "step": 1320
    },
    {
      "epoch": 3.0787037037037037,
      "grad_norm": 4.104844570159912,
      "learning_rate": 4.306020066889632e-05,
      "loss": 0.9761,
      "step": 1330
    },
    {
      "epoch": 3.1018518518518516,
      "grad_norm": 3.598292589187622,
      "learning_rate": 4.297658862876254e-05,
      "loss": 1.085,
      "step": 1340
    },
    {
      "epoch": 3.125,
      "grad_norm": 2.896239757537842,
      "learning_rate": 4.289297658862877e-05,
      "loss": 0.8379,
      "step": 1350
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 3.412970781326294,
      "learning_rate": 4.2809364548494984e-05,
      "loss": 0.951,
      "step": 1360
    },
    {
      "epoch": 3.1712962962962963,
      "grad_norm": 3.7779133319854736,
      "learning_rate": 4.272575250836121e-05,
      "loss": 1.0041,
      "step": 1370
    },
    {
      "epoch": 3.1944444444444446,
      "grad_norm": 3.6078805923461914,
      "learning_rate": 4.2642140468227425e-05,
      "loss": 0.9388,
      "step": 1380
    },
    {
      "epoch": 3.2175925925925926,
      "grad_norm": 3.6012072563171387,
      "learning_rate": 4.255852842809365e-05,
      "loss": 1.0267,
      "step": 1390
    },
    {
      "epoch": 3.240740740740741,
      "grad_norm": 2.0985612869262695,
      "learning_rate": 4.2474916387959866e-05,
      "loss": 1.1458,
      "step": 1400
    },
    {
      "epoch": 3.263888888888889,
      "grad_norm": 4.179809093475342,
      "learning_rate": 4.239130434782609e-05,
      "loss": 0.9873,
      "step": 1410
    },
    {
      "epoch": 3.287037037037037,
      "grad_norm": 3.4531428813934326,
      "learning_rate": 4.230769230769231e-05,
      "loss": 0.7051,
      "step": 1420
    },
    {
      "epoch": 3.310185185185185,
      "grad_norm": 3.0328221321105957,
      "learning_rate": 4.222408026755853e-05,
      "loss": 1.0181,
      "step": 1430
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 2.6042542457580566,
      "learning_rate": 4.214046822742475e-05,
      "loss": 0.8381,
      "step": 1440
    },
    {
      "epoch": 3.3564814814814814,
      "grad_norm": 2.5455408096313477,
      "learning_rate": 4.205685618729097e-05,
      "loss": 0.8681,
      "step": 1450
    },
    {
      "epoch": 3.3796296296296298,
      "grad_norm": 4.5492119789123535,
      "learning_rate": 4.1973244147157196e-05,
      "loss": 0.9264,
      "step": 1460
    },
    {
      "epoch": 3.4027777777777777,
      "grad_norm": 2.508920192718506,
      "learning_rate": 4.188963210702341e-05,
      "loss": 0.8487,
      "step": 1470
    },
    {
      "epoch": 3.425925925925926,
      "grad_norm": 2.825061082839966,
      "learning_rate": 4.180602006688964e-05,
      "loss": 0.9259,
      "step": 1480
    },
    {
      "epoch": 3.449074074074074,
      "grad_norm": 3.290158987045288,
      "learning_rate": 4.1722408026755854e-05,
      "loss": 0.972,
      "step": 1490
    },
    {
      "epoch": 3.4722222222222223,
      "grad_norm": 3.2410340309143066,
      "learning_rate": 4.163879598662208e-05,
      "loss": 0.9525,
      "step": 1500
    },
    {
      "epoch": 3.4953703703703702,
      "grad_norm": 2.8674473762512207,
      "learning_rate": 4.1555183946488295e-05,
      "loss": 0.9141,
      "step": 1510
    },
    {
      "epoch": 3.5185185185185186,
      "grad_norm": 2.4982240200042725,
      "learning_rate": 4.147157190635452e-05,
      "loss": 0.8168,
      "step": 1520
    },
    {
      "epoch": 3.5416666666666665,
      "grad_norm": 2.4083802700042725,
      "learning_rate": 4.1387959866220736e-05,
      "loss": 0.8485,
      "step": 1530
    },
    {
      "epoch": 3.564814814814815,
      "grad_norm": 3.3703970909118652,
      "learning_rate": 4.130434782608696e-05,
      "loss": 0.9552,
      "step": 1540
    },
    {
      "epoch": 3.587962962962963,
      "grad_norm": 4.18258810043335,
      "learning_rate": 4.122073578595318e-05,
      "loss": 0.8869,
      "step": 1550
    },
    {
      "epoch": 3.611111111111111,
      "grad_norm": 3.5647494792938232,
      "learning_rate": 4.11371237458194e-05,
      "loss": 1.1364,
      "step": 1560
    },
    {
      "epoch": 3.6342592592592595,
      "grad_norm": 2.5354604721069336,
      "learning_rate": 4.105351170568562e-05,
      "loss": 0.9072,
      "step": 1570
    },
    {
      "epoch": 3.6574074074074074,
      "grad_norm": 3.0269429683685303,
      "learning_rate": 4.096989966555184e-05,
      "loss": 0.9163,
      "step": 1580
    },
    {
      "epoch": 3.6805555555555554,
      "grad_norm": 2.8245363235473633,
      "learning_rate": 4.0886287625418065e-05,
      "loss": 0.7186,
      "step": 1590
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 3.1107733249664307,
      "learning_rate": 4.080267558528428e-05,
      "loss": 0.9229,
      "step": 1600
    },
    {
      "epoch": 3.726851851851852,
      "grad_norm": 2.1482648849487305,
      "learning_rate": 4.0719063545150506e-05,
      "loss": 0.8747,
      "step": 1610
    },
    {
      "epoch": 3.75,
      "grad_norm": 1.9999445676803589,
      "learning_rate": 4.0635451505016724e-05,
      "loss": 0.8638,
      "step": 1620
    },
    {
      "epoch": 3.773148148148148,
      "grad_norm": 3.515612840652466,
      "learning_rate": 4.055183946488295e-05,
      "loss": 0.8621,
      "step": 1630
    },
    {
      "epoch": 3.7962962962962963,
      "grad_norm": 3.179914951324463,
      "learning_rate": 4.0468227424749165e-05,
      "loss": 1.1065,
      "step": 1640
    },
    {
      "epoch": 3.8194444444444446,
      "grad_norm": 3.9764552116394043,
      "learning_rate": 4.038461538461539e-05,
      "loss": 0.9682,
      "step": 1650
    },
    {
      "epoch": 3.8425925925925926,
      "grad_norm": 2.957718849182129,
      "learning_rate": 4.0301003344481605e-05,
      "loss": 1.0736,
      "step": 1660
    },
    {
      "epoch": 3.8657407407407405,
      "grad_norm": 2.493570327758789,
      "learning_rate": 4.021739130434783e-05,
      "loss": 0.8623,
      "step": 1670
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 3.383776903152466,
      "learning_rate": 4.0133779264214046e-05,
      "loss": 0.9253,
      "step": 1680
    },
    {
      "epoch": 3.912037037037037,
      "grad_norm": 3.246976137161255,
      "learning_rate": 4.005016722408027e-05,
      "loss": 0.9861,
      "step": 1690
    },
    {
      "epoch": 3.935185185185185,
      "grad_norm": 2.3449997901916504,
      "learning_rate": 3.9966555183946494e-05,
      "loss": 0.9096,
      "step": 1700
    },
    {
      "epoch": 3.9583333333333335,
      "grad_norm": 2.3795838356018066,
      "learning_rate": 3.988294314381271e-05,
      "loss": 1.0057,
      "step": 1710
    },
    {
      "epoch": 3.9814814814814814,
      "grad_norm": 2.4993069171905518,
      "learning_rate": 3.9799331103678935e-05,
      "loss": 0.859,
      "step": 1720
    },
    {
      "epoch": 4.00462962962963,
      "grad_norm": 2.6470916271209717,
      "learning_rate": 3.971571906354515e-05,
      "loss": 0.9325,
      "step": 1730
    },
    {
      "epoch": 4.027777777777778,
      "grad_norm": 3.265592098236084,
      "learning_rate": 3.9632107023411376e-05,
      "loss": 0.8562,
      "step": 1740
    },
    {
      "epoch": 4.050925925925926,
      "grad_norm": 1.688959002494812,
      "learning_rate": 3.954849498327759e-05,
      "loss": 0.8138,
      "step": 1750
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 2.7710039615631104,
      "learning_rate": 3.946488294314382e-05,
      "loss": 0.6977,
      "step": 1760
    },
    {
      "epoch": 4.097222222222222,
      "grad_norm": 2.1511452198028564,
      "learning_rate": 3.9381270903010034e-05,
      "loss": 0.8787,
      "step": 1770
    },
    {
      "epoch": 4.12037037037037,
      "grad_norm": 3.007357597351074,
      "learning_rate": 3.929765886287626e-05,
      "loss": 0.8487,
      "step": 1780
    },
    {
      "epoch": 4.143518518518518,
      "grad_norm": 2.3945729732513428,
      "learning_rate": 3.9214046822742475e-05,
      "loss": 0.9408,
      "step": 1790
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 2.9706642627716064,
      "learning_rate": 3.91304347826087e-05,
      "loss": 0.8209,
      "step": 1800
    },
    {
      "epoch": 4.189814814814815,
      "grad_norm": 3.101377010345459,
      "learning_rate": 3.904682274247492e-05,
      "loss": 0.9407,
      "step": 1810
    },
    {
      "epoch": 4.212962962962963,
      "grad_norm": 3.468435764312744,
      "learning_rate": 3.896321070234114e-05,
      "loss": 0.7766,
      "step": 1820
    },
    {
      "epoch": 4.236111111111111,
      "grad_norm": 3.20420241355896,
      "learning_rate": 3.8879598662207364e-05,
      "loss": 0.8796,
      "step": 1830
    },
    {
      "epoch": 4.2592592592592595,
      "grad_norm": 3.253249168395996,
      "learning_rate": 3.879598662207358e-05,
      "loss": 0.873,
      "step": 1840
    },
    {
      "epoch": 4.282407407407407,
      "grad_norm": 3.9147133827209473,
      "learning_rate": 3.8712374581939805e-05,
      "loss": 0.856,
      "step": 1850
    },
    {
      "epoch": 4.305555555555555,
      "grad_norm": 3.0435898303985596,
      "learning_rate": 3.862876254180602e-05,
      "loss": 0.7874,
      "step": 1860
    },
    {
      "epoch": 4.328703703703704,
      "grad_norm": 2.3101634979248047,
      "learning_rate": 3.8545150501672246e-05,
      "loss": 0.925,
      "step": 1870
    },
    {
      "epoch": 4.351851851851852,
      "grad_norm": 2.6362428665161133,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.7748,
      "step": 1880
    },
    {
      "epoch": 4.375,
      "grad_norm": 2.477647304534912,
      "learning_rate": 3.837792642140469e-05,
      "loss": 0.7507,
      "step": 1890
    },
    {
      "epoch": 4.398148148148148,
      "grad_norm": 1.9550143480300903,
      "learning_rate": 3.8294314381270904e-05,
      "loss": 0.8323,
      "step": 1900
    },
    {
      "epoch": 4.421296296296296,
      "grad_norm": 3.673243761062622,
      "learning_rate": 3.821070234113713e-05,
      "loss": 0.9233,
      "step": 1910
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 3.051947593688965,
      "learning_rate": 3.812709030100335e-05,
      "loss": 0.741,
      "step": 1920
    },
    {
      "epoch": 4.467592592592593,
      "grad_norm": 2.2364046573638916,
      "learning_rate": 3.804347826086957e-05,
      "loss": 0.7775,
      "step": 1930
    },
    {
      "epoch": 4.4907407407407405,
      "grad_norm": 3.2413341999053955,
      "learning_rate": 3.795986622073579e-05,
      "loss": 0.9343,
      "step": 1940
    },
    {
      "epoch": 4.513888888888889,
      "grad_norm": 3.087479829788208,
      "learning_rate": 3.787625418060201e-05,
      "loss": 0.8183,
      "step": 1950
    },
    {
      "epoch": 4.537037037037037,
      "grad_norm": 2.289062976837158,
      "learning_rate": 3.7792642140468233e-05,
      "loss": 0.7897,
      "step": 1960
    },
    {
      "epoch": 4.560185185185185,
      "grad_norm": 2.9840500354766846,
      "learning_rate": 3.770903010033445e-05,
      "loss": 0.8901,
      "step": 1970
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 3.8779184818267822,
      "learning_rate": 3.7625418060200674e-05,
      "loss": 0.9672,
      "step": 1980
    },
    {
      "epoch": 4.606481481481482,
      "grad_norm": 4.274030685424805,
      "learning_rate": 3.754180602006689e-05,
      "loss": 0.836,
      "step": 1990
    },
    {
      "epoch": 4.62962962962963,
      "grad_norm": 1.9404855966567993,
      "learning_rate": 3.745819397993311e-05,
      "loss": 0.8647,
      "step": 2000
    },
    {
      "epoch": 4.652777777777778,
      "grad_norm": 2.3986802101135254,
      "learning_rate": 3.737458193979933e-05,
      "loss": 0.7193,
      "step": 2010
    },
    {
      "epoch": 4.675925925925926,
      "grad_norm": 3.1393377780914307,
      "learning_rate": 3.729096989966555e-05,
      "loss": 1.008,
      "step": 2020
    },
    {
      "epoch": 4.699074074074074,
      "grad_norm": 3.691322088241577,
      "learning_rate": 3.7207357859531773e-05,
      "loss": 1.0325,
      "step": 2030
    },
    {
      "epoch": 4.722222222222222,
      "grad_norm": 3.445166826248169,
      "learning_rate": 3.712374581939799e-05,
      "loss": 0.9203,
      "step": 2040
    },
    {
      "epoch": 4.74537037037037,
      "grad_norm": 3.0801029205322266,
      "learning_rate": 3.7040133779264214e-05,
      "loss": 0.8559,
      "step": 2050
    },
    {
      "epoch": 4.768518518518518,
      "grad_norm": 2.516554832458496,
      "learning_rate": 3.695652173913043e-05,
      "loss": 0.8633,
      "step": 2060
    },
    {
      "epoch": 4.791666666666667,
      "grad_norm": 2.360138416290283,
      "learning_rate": 3.6872909698996655e-05,
      "loss": 1.0655,
      "step": 2070
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 3.5479815006256104,
      "learning_rate": 3.678929765886287e-05,
      "loss": 1.0282,
      "step": 2080
    },
    {
      "epoch": 4.837962962962963,
      "grad_norm": 2.730572462081909,
      "learning_rate": 3.6705685618729096e-05,
      "loss": 0.6438,
      "step": 2090
    },
    {
      "epoch": 4.861111111111111,
      "grad_norm": 3.0872950553894043,
      "learning_rate": 3.662207357859532e-05,
      "loss": 0.8415,
      "step": 2100
    },
    {
      "epoch": 4.8842592592592595,
      "grad_norm": 2.1550710201263428,
      "learning_rate": 3.653846153846154e-05,
      "loss": 1.0833,
      "step": 2110
    },
    {
      "epoch": 4.907407407407407,
      "grad_norm": 2.640693426132202,
      "learning_rate": 3.645484949832776e-05,
      "loss": 0.7636,
      "step": 2120
    },
    {
      "epoch": 4.930555555555555,
      "grad_norm": 3.8346266746520996,
      "learning_rate": 3.637123745819398e-05,
      "loss": 1.0468,
      "step": 2130
    },
    {
      "epoch": 4.953703703703704,
      "grad_norm": 2.751497983932495,
      "learning_rate": 3.62876254180602e-05,
      "loss": 0.8589,
      "step": 2140
    },
    {
      "epoch": 4.976851851851852,
      "grad_norm": 3.7744789123535156,
      "learning_rate": 3.620401337792642e-05,
      "loss": 0.9923,
      "step": 2150
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.5356624126434326,
      "learning_rate": 3.612040133779264e-05,
      "loss": 0.8601,
      "step": 2160
    },
    {
      "epoch": 5.023148148148148,
      "grad_norm": 2.0670325756073,
      "learning_rate": 3.603678929765886e-05,
      "loss": 0.6971,
      "step": 2170
    },
    {
      "epoch": 5.046296296296297,
      "grad_norm": 2.5602951049804688,
      "learning_rate": 3.5953177257525084e-05,
      "loss": 0.9337,
      "step": 2180
    },
    {
      "epoch": 5.069444444444445,
      "grad_norm": 2.434312582015991,
      "learning_rate": 3.58695652173913e-05,
      "loss": 0.8404,
      "step": 2190
    },
    {
      "epoch": 5.092592592592593,
      "grad_norm": 3.3633668422698975,
      "learning_rate": 3.5785953177257525e-05,
      "loss": 0.9333,
      "step": 2200
    },
    {
      "epoch": 5.1157407407407405,
      "grad_norm": 3.0872883796691895,
      "learning_rate": 3.570234113712375e-05,
      "loss": 0.9676,
      "step": 2210
    },
    {
      "epoch": 5.138888888888889,
      "grad_norm": 1.6950105428695679,
      "learning_rate": 3.5618729096989966e-05,
      "loss": 0.7771,
      "step": 2220
    },
    {
      "epoch": 5.162037037037037,
      "grad_norm": 2.8752477169036865,
      "learning_rate": 3.553511705685619e-05,
      "loss": 0.7818,
      "step": 2230
    },
    {
      "epoch": 5.185185185185185,
      "grad_norm": 3.6464967727661133,
      "learning_rate": 3.545150501672241e-05,
      "loss": 0.8636,
      "step": 2240
    },
    {
      "epoch": 5.208333333333333,
      "grad_norm": 2.7767276763916016,
      "learning_rate": 3.536789297658863e-05,
      "loss": 0.8767,
      "step": 2250
    },
    {
      "epoch": 5.231481481481482,
      "grad_norm": 2.742238759994507,
      "learning_rate": 3.528428093645485e-05,
      "loss": 0.8757,
      "step": 2260
    },
    {
      "epoch": 5.25462962962963,
      "grad_norm": 2.6371450424194336,
      "learning_rate": 3.520066889632107e-05,
      "loss": 0.8276,
      "step": 2270
    },
    {
      "epoch": 5.277777777777778,
      "grad_norm": 3.0992770195007324,
      "learning_rate": 3.511705685618729e-05,
      "loss": 0.9361,
      "step": 2280
    },
    {
      "epoch": 5.300925925925926,
      "grad_norm": 2.2697432041168213,
      "learning_rate": 3.503344481605351e-05,
      "loss": 0.8304,
      "step": 2290
    },
    {
      "epoch": 5.324074074074074,
      "grad_norm": 3.0963664054870605,
      "learning_rate": 3.494983277591973e-05,
      "loss": 0.8738,
      "step": 2300
    },
    {
      "epoch": 5.347222222222222,
      "grad_norm": 3.941314220428467,
      "learning_rate": 3.4866220735785954e-05,
      "loss": 0.7826,
      "step": 2310
    },
    {
      "epoch": 5.37037037037037,
      "grad_norm": 2.9732279777526855,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.0488,
      "step": 2320
    },
    {
      "epoch": 5.393518518518518,
      "grad_norm": 3.090991973876953,
      "learning_rate": 3.4698996655518395e-05,
      "loss": 0.7101,
      "step": 2330
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 2.8513569831848145,
      "learning_rate": 3.461538461538462e-05,
      "loss": 0.7231,
      "step": 2340
    },
    {
      "epoch": 5.439814814814815,
      "grad_norm": 2.2441036701202393,
      "learning_rate": 3.4531772575250836e-05,
      "loss": 0.7526,
      "step": 2350
    },
    {
      "epoch": 5.462962962962963,
      "grad_norm": 2.5150647163391113,
      "learning_rate": 3.444816053511706e-05,
      "loss": 0.6799,
      "step": 2360
    },
    {
      "epoch": 5.486111111111111,
      "grad_norm": 3.018792152404785,
      "learning_rate": 3.436454849498328e-05,
      "loss": 0.8867,
      "step": 2370
    },
    {
      "epoch": 5.5092592592592595,
      "grad_norm": 3.2789993286132812,
      "learning_rate": 3.42809364548495e-05,
      "loss": 0.8169,
      "step": 2380
    },
    {
      "epoch": 5.532407407407407,
      "grad_norm": 3.145329713821411,
      "learning_rate": 3.419732441471572e-05,
      "loss": 0.7692,
      "step": 2390
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 3.5895912647247314,
      "learning_rate": 3.411371237458194e-05,
      "loss": 0.6768,
      "step": 2400
    },
    {
      "epoch": 5.578703703703704,
      "grad_norm": 1.469583511352539,
      "learning_rate": 3.403010033444816e-05,
      "loss": 0.8485,
      "step": 2410
    },
    {
      "epoch": 5.601851851851852,
      "grad_norm": 2.863189935684204,
      "learning_rate": 3.394648829431438e-05,
      "loss": 0.7433,
      "step": 2420
    },
    {
      "epoch": 5.625,
      "grad_norm": 3.003680944442749,
      "learning_rate": 3.3862876254180606e-05,
      "loss": 0.7074,
      "step": 2430
    },
    {
      "epoch": 5.648148148148148,
      "grad_norm": 2.582984209060669,
      "learning_rate": 3.3779264214046823e-05,
      "loss": 0.6294,
      "step": 2440
    },
    {
      "epoch": 5.671296296296296,
      "grad_norm": 2.6360108852386475,
      "learning_rate": 3.369565217391305e-05,
      "loss": 0.812,
      "step": 2450
    },
    {
      "epoch": 5.694444444444445,
      "grad_norm": 2.6395721435546875,
      "learning_rate": 3.3612040133779264e-05,
      "loss": 0.697,
      "step": 2460
    },
    {
      "epoch": 5.717592592592593,
      "grad_norm": 2.695880889892578,
      "learning_rate": 3.352842809364549e-05,
      "loss": 0.8183,
      "step": 2470
    },
    {
      "epoch": 5.7407407407407405,
      "grad_norm": 2.4438281059265137,
      "learning_rate": 3.3444816053511705e-05,
      "loss": 0.824,
      "step": 2480
    },
    {
      "epoch": 5.763888888888889,
      "grad_norm": 2.9297327995300293,
      "learning_rate": 3.336120401337793e-05,
      "loss": 0.7824,
      "step": 2490
    },
    {
      "epoch": 5.787037037037037,
      "grad_norm": 2.384345531463623,
      "learning_rate": 3.3277591973244146e-05,
      "loss": 0.7581,
      "step": 2500
    },
    {
      "epoch": 5.810185185185185,
      "grad_norm": 3.6524226665496826,
      "learning_rate": 3.319397993311037e-05,
      "loss": 0.8711,
      "step": 2510
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 2.055589437484741,
      "learning_rate": 3.311036789297659e-05,
      "loss": 0.6994,
      "step": 2520
    },
    {
      "epoch": 5.856481481481482,
      "grad_norm": 4.122402191162109,
      "learning_rate": 3.302675585284281e-05,
      "loss": 0.9013,
      "step": 2530
    },
    {
      "epoch": 5.87962962962963,
      "grad_norm": 3.2198123931884766,
      "learning_rate": 3.2943143812709035e-05,
      "loss": 0.7685,
      "step": 2540
    },
    {
      "epoch": 5.902777777777778,
      "grad_norm": 3.0160787105560303,
      "learning_rate": 3.285953177257525e-05,
      "loss": 0.9866,
      "step": 2550
    },
    {
      "epoch": 5.925925925925926,
      "grad_norm": 3.000971555709839,
      "learning_rate": 3.2775919732441476e-05,
      "loss": 0.8926,
      "step": 2560
    },
    {
      "epoch": 5.949074074074074,
      "grad_norm": 2.6328721046447754,
      "learning_rate": 3.269230769230769e-05,
      "loss": 0.7235,
      "step": 2570
    },
    {
      "epoch": 5.972222222222222,
      "grad_norm": 3.4582693576812744,
      "learning_rate": 3.260869565217392e-05,
      "loss": 0.9145,
      "step": 2580
    },
    {
      "epoch": 5.99537037037037,
      "grad_norm": 2.9281373023986816,
      "learning_rate": 3.2525083612040134e-05,
      "loss": 0.8515,
      "step": 2590
    },
    {
      "epoch": 6.018518518518518,
      "grad_norm": 3.938718557357788,
      "learning_rate": 3.244147157190636e-05,
      "loss": 0.9483,
      "step": 2600
    },
    {
      "epoch": 6.041666666666667,
      "grad_norm": 2.343592405319214,
      "learning_rate": 3.2357859531772575e-05,
      "loss": 0.8153,
      "step": 2610
    },
    {
      "epoch": 6.064814814814815,
      "grad_norm": 3.238844394683838,
      "learning_rate": 3.22742474916388e-05,
      "loss": 0.8741,
      "step": 2620
    },
    {
      "epoch": 6.087962962962963,
      "grad_norm": 3.0914742946624756,
      "learning_rate": 3.2190635451505016e-05,
      "loss": 0.8604,
      "step": 2630
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 2.5368220806121826,
      "learning_rate": 3.210702341137124e-05,
      "loss": 0.8883,
      "step": 2640
    },
    {
      "epoch": 6.1342592592592595,
      "grad_norm": 3.765869140625,
      "learning_rate": 3.2023411371237464e-05,
      "loss": 0.7999,
      "step": 2650
    },
    {
      "epoch": 6.157407407407407,
      "grad_norm": 2.8428895473480225,
      "learning_rate": 3.193979933110368e-05,
      "loss": 0.8281,
      "step": 2660
    },
    {
      "epoch": 6.180555555555555,
      "grad_norm": 2.983924150466919,
      "learning_rate": 3.1856187290969905e-05,
      "loss": 0.7414,
      "step": 2670
    },
    {
      "epoch": 6.203703703703703,
      "grad_norm": 3.3183271884918213,
      "learning_rate": 3.177257525083612e-05,
      "loss": 0.9879,
      "step": 2680
    },
    {
      "epoch": 6.226851851851852,
      "grad_norm": 2.116258144378662,
      "learning_rate": 3.1688963210702346e-05,
      "loss": 0.8438,
      "step": 2690
    },
    {
      "epoch": 6.25,
      "grad_norm": 2.6310346126556396,
      "learning_rate": 3.160535117056856e-05,
      "loss": 0.8177,
      "step": 2700
    },
    {
      "epoch": 6.273148148148148,
      "grad_norm": 3.533247470855713,
      "learning_rate": 3.152173913043479e-05,
      "loss": 1.0056,
      "step": 2710
    },
    {
      "epoch": 6.296296296296296,
      "grad_norm": 2.224246025085449,
      "learning_rate": 3.1438127090301004e-05,
      "loss": 0.6875,
      "step": 2720
    },
    {
      "epoch": 6.319444444444445,
      "grad_norm": 2.391160011291504,
      "learning_rate": 3.135451505016723e-05,
      "loss": 0.7627,
      "step": 2730
    },
    {
      "epoch": 6.342592592592593,
      "grad_norm": 2.6381404399871826,
      "learning_rate": 3.1270903010033445e-05,
      "loss": 0.7458,
      "step": 2740
    },
    {
      "epoch": 6.3657407407407405,
      "grad_norm": 2.7786881923675537,
      "learning_rate": 3.118729096989967e-05,
      "loss": 0.6814,
      "step": 2750
    },
    {
      "epoch": 6.388888888888889,
      "grad_norm": 3.583922863006592,
      "learning_rate": 3.110367892976589e-05,
      "loss": 0.8556,
      "step": 2760
    },
    {
      "epoch": 6.412037037037037,
      "grad_norm": 2.5100691318511963,
      "learning_rate": 3.102006688963211e-05,
      "loss": 0.7248,
      "step": 2770
    },
    {
      "epoch": 6.435185185185185,
      "grad_norm": 2.1517252922058105,
      "learning_rate": 3.0936454849498333e-05,
      "loss": 0.7143,
      "step": 2780
    },
    {
      "epoch": 6.458333333333333,
      "grad_norm": 2.01249098777771,
      "learning_rate": 3.085284280936455e-05,
      "loss": 0.6036,
      "step": 2790
    },
    {
      "epoch": 6.481481481481482,
      "grad_norm": 2.097574472427368,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 0.5588,
      "step": 2800
    },
    {
      "epoch": 6.50462962962963,
      "grad_norm": 3.0559191703796387,
      "learning_rate": 3.068561872909699e-05,
      "loss": 0.891,
      "step": 2810
    },
    {
      "epoch": 6.527777777777778,
      "grad_norm": 2.730104684829712,
      "learning_rate": 3.0602006688963215e-05,
      "loss": 0.7558,
      "step": 2820
    },
    {
      "epoch": 6.550925925925926,
      "grad_norm": 2.6901438236236572,
      "learning_rate": 3.051839464882943e-05,
      "loss": 0.7445,
      "step": 2830
    },
    {
      "epoch": 6.574074074074074,
      "grad_norm": 3.765033006668091,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 0.8312,
      "step": 2840
    },
    {
      "epoch": 6.597222222222222,
      "grad_norm": 3.0062317848205566,
      "learning_rate": 3.0351170568561877e-05,
      "loss": 0.6094,
      "step": 2850
    },
    {
      "epoch": 6.62037037037037,
      "grad_norm": 1.6791248321533203,
      "learning_rate": 3.0267558528428097e-05,
      "loss": 0.7264,
      "step": 2860
    },
    {
      "epoch": 6.643518518518518,
      "grad_norm": 2.6932435035705566,
      "learning_rate": 3.0183946488294318e-05,
      "loss": 0.6188,
      "step": 2870
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 2.743490219116211,
      "learning_rate": 3.0100334448160538e-05,
      "loss": 0.7706,
      "step": 2880
    },
    {
      "epoch": 6.689814814814815,
      "grad_norm": 2.711817502975464,
      "learning_rate": 3.001672240802676e-05,
      "loss": 0.8703,
      "step": 2890
    },
    {
      "epoch": 6.712962962962963,
      "grad_norm": 2.992060899734497,
      "learning_rate": 2.993311036789298e-05,
      "loss": 0.7687,
      "step": 2900
    },
    {
      "epoch": 6.736111111111111,
      "grad_norm": 2.457585573196411,
      "learning_rate": 2.98494983277592e-05,
      "loss": 0.7081,
      "step": 2910
    },
    {
      "epoch": 6.7592592592592595,
      "grad_norm": 2.169208288192749,
      "learning_rate": 2.9765886287625424e-05,
      "loss": 0.7053,
      "step": 2920
    },
    {
      "epoch": 6.782407407407407,
      "grad_norm": 2.6700801849365234,
      "learning_rate": 2.9682274247491644e-05,
      "loss": 0.7669,
      "step": 2930
    },
    {
      "epoch": 6.805555555555555,
      "grad_norm": 3.4393386840820312,
      "learning_rate": 2.9598662207357864e-05,
      "loss": 0.9626,
      "step": 2940
    },
    {
      "epoch": 6.828703703703704,
      "grad_norm": 1.9159753322601318,
      "learning_rate": 2.9515050167224085e-05,
      "loss": 0.6414,
      "step": 2950
    },
    {
      "epoch": 6.851851851851852,
      "grad_norm": 2.392327308654785,
      "learning_rate": 2.9431438127090305e-05,
      "loss": 0.7897,
      "step": 2960
    },
    {
      "epoch": 6.875,
      "grad_norm": 3.7058136463165283,
      "learning_rate": 2.9347826086956526e-05,
      "loss": 0.73,
      "step": 2970
    },
    {
      "epoch": 6.898148148148148,
      "grad_norm": 2.6888363361358643,
      "learning_rate": 2.9264214046822746e-05,
      "loss": 0.6982,
      "step": 2980
    },
    {
      "epoch": 6.921296296296296,
      "grad_norm": 4.219508171081543,
      "learning_rate": 2.9180602006688967e-05,
      "loss": 0.7373,
      "step": 2990
    },
    {
      "epoch": 6.944444444444445,
      "grad_norm": 2.8528451919555664,
      "learning_rate": 2.9096989966555184e-05,
      "loss": 0.7649,
      "step": 3000
    },
    {
      "epoch": 6.967592592592593,
      "grad_norm": 3.162774085998535,
      "learning_rate": 2.9013377926421404e-05,
      "loss": 0.6198,
      "step": 3010
    },
    {
      "epoch": 6.9907407407407405,
      "grad_norm": 3.4989030361175537,
      "learning_rate": 2.8929765886287625e-05,
      "loss": 0.8104,
      "step": 3020
    },
    {
      "epoch": 7.013888888888889,
      "grad_norm": 2.7140557765960693,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 0.6694,
      "step": 3030
    },
    {
      "epoch": 7.037037037037037,
      "grad_norm": 2.282637119293213,
      "learning_rate": 2.8762541806020066e-05,
      "loss": 0.8737,
      "step": 3040
    },
    {
      "epoch": 7.060185185185185,
      "grad_norm": 2.8965883255004883,
      "learning_rate": 2.8678929765886286e-05,
      "loss": 0.8488,
      "step": 3050
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 3.1623318195343018,
      "learning_rate": 2.8595317725752507e-05,
      "loss": 0.8902,
      "step": 3060
    },
    {
      "epoch": 7.106481481481482,
      "grad_norm": 2.4657034873962402,
      "learning_rate": 2.8511705685618727e-05,
      "loss": 0.6712,
      "step": 3070
    },
    {
      "epoch": 7.12962962962963,
      "grad_norm": 2.6897289752960205,
      "learning_rate": 2.8428093645484948e-05,
      "loss": 0.8779,
      "step": 3080
    },
    {
      "epoch": 7.152777777777778,
      "grad_norm": 1.9471371173858643,
      "learning_rate": 2.834448160535117e-05,
      "loss": 0.7005,
      "step": 3090
    },
    {
      "epoch": 7.175925925925926,
      "grad_norm": 2.3923752307891846,
      "learning_rate": 2.826086956521739e-05,
      "loss": 0.7258,
      "step": 3100
    },
    {
      "epoch": 7.199074074074074,
      "grad_norm": 2.6920392513275146,
      "learning_rate": 2.8177257525083613e-05,
      "loss": 0.6985,
      "step": 3110
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 3.2398922443389893,
      "learning_rate": 2.8093645484949833e-05,
      "loss": 0.7199,
      "step": 3120
    },
    {
      "epoch": 7.24537037037037,
      "grad_norm": 1.8565962314605713,
      "learning_rate": 2.8010033444816054e-05,
      "loss": 0.6736,
      "step": 3130
    },
    {
      "epoch": 7.268518518518518,
      "grad_norm": 2.7106709480285645,
      "learning_rate": 2.7926421404682274e-05,
      "loss": 0.5427,
      "step": 3140
    },
    {
      "epoch": 7.291666666666667,
      "grad_norm": 2.0659339427948,
      "learning_rate": 2.7842809364548495e-05,
      "loss": 0.6831,
      "step": 3150
    },
    {
      "epoch": 7.314814814814815,
      "grad_norm": 2.769925832748413,
      "learning_rate": 2.7759197324414715e-05,
      "loss": 0.7059,
      "step": 3160
    },
    {
      "epoch": 7.337962962962963,
      "grad_norm": 2.9856483936309814,
      "learning_rate": 2.7675585284280936e-05,
      "loss": 0.7979,
      "step": 3170
    },
    {
      "epoch": 7.361111111111111,
      "grad_norm": 2.324808359146118,
      "learning_rate": 2.7591973244147156e-05,
      "loss": 0.7876,
      "step": 3180
    },
    {
      "epoch": 7.3842592592592595,
      "grad_norm": 2.648782968521118,
      "learning_rate": 2.7508361204013377e-05,
      "loss": 0.6326,
      "step": 3190
    },
    {
      "epoch": 7.407407407407407,
      "grad_norm": 3.426290273666382,
      "learning_rate": 2.7424749163879597e-05,
      "loss": 0.9557,
      "step": 3200
    },
    {
      "epoch": 7.430555555555555,
      "grad_norm": 1.7351423501968384,
      "learning_rate": 2.7341137123745818e-05,
      "loss": 0.6555,
      "step": 3210
    },
    {
      "epoch": 7.453703703703704,
      "grad_norm": 1.9386460781097412,
      "learning_rate": 2.725752508361204e-05,
      "loss": 0.5841,
      "step": 3220
    },
    {
      "epoch": 7.476851851851852,
      "grad_norm": 2.3722922801971436,
      "learning_rate": 2.7173913043478262e-05,
      "loss": 0.693,
      "step": 3230
    },
    {
      "epoch": 7.5,
      "grad_norm": 2.432849645614624,
      "learning_rate": 2.7090301003344482e-05,
      "loss": 0.8542,
      "step": 3240
    },
    {
      "epoch": 7.523148148148148,
      "grad_norm": 3.2384181022644043,
      "learning_rate": 2.7006688963210703e-05,
      "loss": 0.8521,
      "step": 3250
    },
    {
      "epoch": 7.546296296296296,
      "grad_norm": 2.116774797439575,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 0.6031,
      "step": 3260
    },
    {
      "epoch": 7.569444444444445,
      "grad_norm": 1.8237541913986206,
      "learning_rate": 2.6839464882943144e-05,
      "loss": 0.7008,
      "step": 3270
    },
    {
      "epoch": 7.592592592592593,
      "grad_norm": 2.7780251502990723,
      "learning_rate": 2.6755852842809364e-05,
      "loss": 0.7196,
      "step": 3280
    },
    {
      "epoch": 7.6157407407407405,
      "grad_norm": 2.8358967304229736,
      "learning_rate": 2.6672240802675585e-05,
      "loss": 0.6013,
      "step": 3290
    },
    {
      "epoch": 7.638888888888889,
      "grad_norm": 2.6147429943084717,
      "learning_rate": 2.6588628762541805e-05,
      "loss": 0.6354,
      "step": 3300
    },
    {
      "epoch": 7.662037037037037,
      "grad_norm": 2.631594657897949,
      "learning_rate": 2.6505016722408026e-05,
      "loss": 0.6929,
      "step": 3310
    },
    {
      "epoch": 7.685185185185185,
      "grad_norm": 2.6606266498565674,
      "learning_rate": 2.6421404682274246e-05,
      "loss": 0.7002,
      "step": 3320
    },
    {
      "epoch": 7.708333333333333,
      "grad_norm": 3.1919031143188477,
      "learning_rate": 2.633779264214047e-05,
      "loss": 0.8924,
      "step": 3330
    },
    {
      "epoch": 7.731481481481482,
      "grad_norm": 1.524383783340454,
      "learning_rate": 2.625418060200669e-05,
      "loss": 0.5688,
      "step": 3340
    },
    {
      "epoch": 7.75462962962963,
      "grad_norm": 2.4434831142425537,
      "learning_rate": 2.617056856187291e-05,
      "loss": 0.7755,
      "step": 3350
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 3.0420942306518555,
      "learning_rate": 2.608695652173913e-05,
      "loss": 0.8619,
      "step": 3360
    },
    {
      "epoch": 7.800925925925926,
      "grad_norm": 2.222473621368408,
      "learning_rate": 2.6003344481605352e-05,
      "loss": 0.6434,
      "step": 3370
    },
    {
      "epoch": 7.824074074074074,
      "grad_norm": 3.2889201641082764,
      "learning_rate": 2.5919732441471573e-05,
      "loss": 0.9242,
      "step": 3380
    },
    {
      "epoch": 7.847222222222222,
      "grad_norm": 3.3798508644104004,
      "learning_rate": 2.5836120401337793e-05,
      "loss": 0.8085,
      "step": 3390
    },
    {
      "epoch": 7.87037037037037,
      "grad_norm": 3.257087230682373,
      "learning_rate": 2.5752508361204013e-05,
      "loss": 0.6141,
      "step": 3400
    },
    {
      "epoch": 7.893518518518518,
      "grad_norm": 2.261188268661499,
      "learning_rate": 2.5668896321070234e-05,
      "loss": 0.7024,
      "step": 3410
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 1.352372646331787,
      "learning_rate": 2.5585284280936454e-05,
      "loss": 0.6427,
      "step": 3420
    },
    {
      "epoch": 7.939814814814815,
      "grad_norm": 2.4943480491638184,
      "learning_rate": 2.5501672240802675e-05,
      "loss": 0.85,
      "step": 3430
    },
    {
      "epoch": 7.962962962962963,
      "grad_norm": 2.7165133953094482,
      "learning_rate": 2.54180602006689e-05,
      "loss": 0.7262,
      "step": 3440
    },
    {
      "epoch": 7.986111111111111,
      "grad_norm": 1.974458932876587,
      "learning_rate": 2.533444816053512e-05,
      "loss": 0.7805,
      "step": 3450
    },
    {
      "epoch": 8.00925925925926,
      "grad_norm": 2.4791524410247803,
      "learning_rate": 2.525083612040134e-05,
      "loss": 0.7234,
      "step": 3460
    },
    {
      "epoch": 8.032407407407407,
      "grad_norm": 3.3448293209075928,
      "learning_rate": 2.516722408026756e-05,
      "loss": 0.7938,
      "step": 3470
    },
    {
      "epoch": 8.055555555555555,
      "grad_norm": 3.0404105186462402,
      "learning_rate": 2.508361204013378e-05,
      "loss": 0.7197,
      "step": 3480
    },
    {
      "epoch": 8.078703703703704,
      "grad_norm": 2.9941153526306152,
      "learning_rate": 2.5e-05,
      "loss": 0.6772,
      "step": 3490
    },
    {
      "epoch": 8.101851851851851,
      "grad_norm": 2.602038860321045,
      "learning_rate": 2.491638795986622e-05,
      "loss": 0.6716,
      "step": 3500
    },
    {
      "epoch": 8.125,
      "grad_norm": 2.751274585723877,
      "learning_rate": 2.4832775919732442e-05,
      "loss": 0.7797,
      "step": 3510
    },
    {
      "epoch": 8.148148148148149,
      "grad_norm": 1.8258410692214966,
      "learning_rate": 2.4749163879598663e-05,
      "loss": 0.7337,
      "step": 3520
    },
    {
      "epoch": 8.171296296296296,
      "grad_norm": 2.9351749420166016,
      "learning_rate": 2.4665551839464883e-05,
      "loss": 0.599,
      "step": 3530
    },
    {
      "epoch": 8.194444444444445,
      "grad_norm": 2.8740367889404297,
      "learning_rate": 2.4581939799331104e-05,
      "loss": 0.7322,
      "step": 3540
    },
    {
      "epoch": 8.217592592592593,
      "grad_norm": 2.142906665802002,
      "learning_rate": 2.4498327759197327e-05,
      "loss": 0.5973,
      "step": 3550
    },
    {
      "epoch": 8.24074074074074,
      "grad_norm": 2.551619291305542,
      "learning_rate": 2.4414715719063548e-05,
      "loss": 0.7533,
      "step": 3560
    },
    {
      "epoch": 8.26388888888889,
      "grad_norm": 2.844942331314087,
      "learning_rate": 2.433110367892977e-05,
      "loss": 0.6445,
      "step": 3570
    },
    {
      "epoch": 8.287037037037036,
      "grad_norm": 1.7676725387573242,
      "learning_rate": 2.424749163879599e-05,
      "loss": 0.8068,
      "step": 3580
    },
    {
      "epoch": 8.310185185185185,
      "grad_norm": 3.020721197128296,
      "learning_rate": 2.416387959866221e-05,
      "loss": 0.6976,
      "step": 3590
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 3.561168909072876,
      "learning_rate": 2.408026755852843e-05,
      "loss": 0.7164,
      "step": 3600
    },
    {
      "epoch": 8.356481481481481,
      "grad_norm": 2.750546455383301,
      "learning_rate": 2.399665551839465e-05,
      "loss": 0.5648,
      "step": 3610
    },
    {
      "epoch": 8.37962962962963,
      "grad_norm": 2.694807529449463,
      "learning_rate": 2.391304347826087e-05,
      "loss": 0.7018,
      "step": 3620
    },
    {
      "epoch": 8.402777777777779,
      "grad_norm": 2.708502769470215,
      "learning_rate": 2.382943143812709e-05,
      "loss": 0.769,
      "step": 3630
    },
    {
      "epoch": 8.425925925925926,
      "grad_norm": 1.7930662631988525,
      "learning_rate": 2.3745819397993312e-05,
      "loss": 0.6581,
      "step": 3640
    },
    {
      "epoch": 8.449074074074074,
      "grad_norm": 2.7119264602661133,
      "learning_rate": 2.3662207357859532e-05,
      "loss": 0.6647,
      "step": 3650
    },
    {
      "epoch": 8.472222222222221,
      "grad_norm": 2.9807698726654053,
      "learning_rate": 2.3578595317725756e-05,
      "loss": 0.6102,
      "step": 3660
    },
    {
      "epoch": 8.49537037037037,
      "grad_norm": 2.9479787349700928,
      "learning_rate": 2.3494983277591977e-05,
      "loss": 0.5723,
      "step": 3670
    },
    {
      "epoch": 8.518518518518519,
      "grad_norm": 2.707763671875,
      "learning_rate": 2.3411371237458197e-05,
      "loss": 0.783,
      "step": 3680
    },
    {
      "epoch": 8.541666666666666,
      "grad_norm": 2.919076442718506,
      "learning_rate": 2.3327759197324418e-05,
      "loss": 0.8407,
      "step": 3690
    },
    {
      "epoch": 8.564814814814815,
      "grad_norm": 2.840359926223755,
      "learning_rate": 2.3244147157190638e-05,
      "loss": 0.7828,
      "step": 3700
    },
    {
      "epoch": 8.587962962962964,
      "grad_norm": 2.359250783920288,
      "learning_rate": 2.316053511705686e-05,
      "loss": 0.7296,
      "step": 3710
    },
    {
      "epoch": 8.61111111111111,
      "grad_norm": 3.203779458999634,
      "learning_rate": 2.307692307692308e-05,
      "loss": 0.7266,
      "step": 3720
    },
    {
      "epoch": 8.63425925925926,
      "grad_norm": 2.519300699234009,
      "learning_rate": 2.29933110367893e-05,
      "loss": 0.7672,
      "step": 3730
    },
    {
      "epoch": 8.657407407407408,
      "grad_norm": 2.9184718132019043,
      "learning_rate": 2.2909698996655517e-05,
      "loss": 0.6273,
      "step": 3740
    },
    {
      "epoch": 8.680555555555555,
      "grad_norm": 2.8042118549346924,
      "learning_rate": 2.282608695652174e-05,
      "loss": 0.6777,
      "step": 3750
    },
    {
      "epoch": 8.703703703703704,
      "grad_norm": 2.064641237258911,
      "learning_rate": 2.274247491638796e-05,
      "loss": 0.677,
      "step": 3760
    },
    {
      "epoch": 8.726851851851851,
      "grad_norm": 2.57387113571167,
      "learning_rate": 2.265886287625418e-05,
      "loss": 0.7646,
      "step": 3770
    },
    {
      "epoch": 8.75,
      "grad_norm": 2.5898630619049072,
      "learning_rate": 2.2575250836120402e-05,
      "loss": 0.8328,
      "step": 3780
    },
    {
      "epoch": 8.773148148148149,
      "grad_norm": 2.550187826156616,
      "learning_rate": 2.2491638795986622e-05,
      "loss": 0.7665,
      "step": 3790
    },
    {
      "epoch": 8.796296296296296,
      "grad_norm": 1.7502557039260864,
      "learning_rate": 2.2408026755852843e-05,
      "loss": 0.6377,
      "step": 3800
    },
    {
      "epoch": 8.819444444444445,
      "grad_norm": 2.3857617378234863,
      "learning_rate": 2.2324414715719063e-05,
      "loss": 0.5987,
      "step": 3810
    },
    {
      "epoch": 8.842592592592592,
      "grad_norm": 3.6526875495910645,
      "learning_rate": 2.2240802675585284e-05,
      "loss": 0.664,
      "step": 3820
    },
    {
      "epoch": 8.86574074074074,
      "grad_norm": 2.0672624111175537,
      "learning_rate": 2.2157190635451504e-05,
      "loss": 0.5701,
      "step": 3830
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 1.9118658304214478,
      "learning_rate": 2.2073578595317725e-05,
      "loss": 0.5984,
      "step": 3840
    },
    {
      "epoch": 8.912037037037036,
      "grad_norm": 2.5288467407226562,
      "learning_rate": 2.1989966555183945e-05,
      "loss": 0.7233,
      "step": 3850
    },
    {
      "epoch": 8.935185185185185,
      "grad_norm": 3.140394926071167,
      "learning_rate": 2.190635451505017e-05,
      "loss": 0.628,
      "step": 3860
    },
    {
      "epoch": 8.958333333333334,
      "grad_norm": 2.640963554382324,
      "learning_rate": 2.182274247491639e-05,
      "loss": 0.764,
      "step": 3870
    },
    {
      "epoch": 8.981481481481481,
      "grad_norm": 2.6286473274230957,
      "learning_rate": 2.173913043478261e-05,
      "loss": 0.6206,
      "step": 3880
    },
    {
      "epoch": 9.00462962962963,
      "grad_norm": 2.7065341472625732,
      "learning_rate": 2.165551839464883e-05,
      "loss": 0.7293,
      "step": 3890
    },
    {
      "epoch": 9.027777777777779,
      "grad_norm": 2.0260443687438965,
      "learning_rate": 2.157190635451505e-05,
      "loss": 0.7212,
      "step": 3900
    },
    {
      "epoch": 9.050925925925926,
      "grad_norm": 2.45489764213562,
      "learning_rate": 2.148829431438127e-05,
      "loss": 0.7164,
      "step": 3910
    },
    {
      "epoch": 9.074074074074074,
      "grad_norm": 2.3538968563079834,
      "learning_rate": 2.1404682274247492e-05,
      "loss": 0.6842,
      "step": 3920
    },
    {
      "epoch": 9.097222222222221,
      "grad_norm": 2.9414074420928955,
      "learning_rate": 2.1321070234113713e-05,
      "loss": 0.7178,
      "step": 3930
    },
    {
      "epoch": 9.12037037037037,
      "grad_norm": 2.511544704437256,
      "learning_rate": 2.1237458193979933e-05,
      "loss": 0.6307,
      "step": 3940
    },
    {
      "epoch": 9.143518518518519,
      "grad_norm": 2.536522388458252,
      "learning_rate": 2.1153846153846154e-05,
      "loss": 0.5279,
      "step": 3950
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 1.4901186227798462,
      "learning_rate": 2.1070234113712374e-05,
      "loss": 0.8036,
      "step": 3960
    },
    {
      "epoch": 9.189814814814815,
      "grad_norm": 2.296722412109375,
      "learning_rate": 2.0986622073578598e-05,
      "loss": 0.6997,
      "step": 3970
    },
    {
      "epoch": 9.212962962962964,
      "grad_norm": 3.6132426261901855,
      "learning_rate": 2.090301003344482e-05,
      "loss": 0.6376,
      "step": 3980
    },
    {
      "epoch": 9.23611111111111,
      "grad_norm": 2.5060343742370605,
      "learning_rate": 2.081939799331104e-05,
      "loss": 0.6097,
      "step": 3990
    },
    {
      "epoch": 9.25925925925926,
      "grad_norm": 3.24226450920105,
      "learning_rate": 2.073578595317726e-05,
      "loss": 0.7468,
      "step": 4000
    },
    {
      "epoch": 9.282407407407407,
      "grad_norm": 2.6826624870300293,
      "learning_rate": 2.065217391304348e-05,
      "loss": 0.6465,
      "step": 4010
    },
    {
      "epoch": 9.305555555555555,
      "grad_norm": 2.3647947311401367,
      "learning_rate": 2.05685618729097e-05,
      "loss": 0.6198,
      "step": 4020
    },
    {
      "epoch": 9.328703703703704,
      "grad_norm": 2.600127935409546,
      "learning_rate": 2.048494983277592e-05,
      "loss": 0.623,
      "step": 4030
    },
    {
      "epoch": 9.351851851851851,
      "grad_norm": 3.1076791286468506,
      "learning_rate": 2.040133779264214e-05,
      "loss": 0.602,
      "step": 4040
    },
    {
      "epoch": 9.375,
      "grad_norm": 1.8538808822631836,
      "learning_rate": 2.0317725752508362e-05,
      "loss": 0.6754,
      "step": 4050
    },
    {
      "epoch": 9.398148148148149,
      "grad_norm": 2.344292402267456,
      "learning_rate": 2.0234113712374582e-05,
      "loss": 0.702,
      "step": 4060
    },
    {
      "epoch": 9.421296296296296,
      "grad_norm": 2.0725226402282715,
      "learning_rate": 2.0150501672240803e-05,
      "loss": 0.6114,
      "step": 4070
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 1.320807695388794,
      "learning_rate": 2.0066889632107023e-05,
      "loss": 0.696,
      "step": 4080
    },
    {
      "epoch": 9.467592592592593,
      "grad_norm": 1.987113118171692,
      "learning_rate": 1.9983277591973247e-05,
      "loss": 0.5658,
      "step": 4090
    },
    {
      "epoch": 9.49074074074074,
      "grad_norm": 1.8642523288726807,
      "learning_rate": 1.9899665551839468e-05,
      "loss": 0.5755,
      "step": 4100
    },
    {
      "epoch": 9.51388888888889,
      "grad_norm": 2.888974666595459,
      "learning_rate": 1.9816053511705688e-05,
      "loss": 0.6325,
      "step": 4110
    },
    {
      "epoch": 9.537037037037036,
      "grad_norm": 2.46835994720459,
      "learning_rate": 1.973244147157191e-05,
      "loss": 0.6492,
      "step": 4120
    },
    {
      "epoch": 9.560185185185185,
      "grad_norm": 2.6060492992401123,
      "learning_rate": 1.964882943143813e-05,
      "loss": 0.6512,
      "step": 4130
    },
    {
      "epoch": 9.583333333333334,
      "grad_norm": 4.017999649047852,
      "learning_rate": 1.956521739130435e-05,
      "loss": 0.7905,
      "step": 4140
    },
    {
      "epoch": 9.606481481481481,
      "grad_norm": 3.093925714492798,
      "learning_rate": 1.948160535117057e-05,
      "loss": 0.8283,
      "step": 4150
    },
    {
      "epoch": 9.62962962962963,
      "grad_norm": 3.306389093399048,
      "learning_rate": 1.939799331103679e-05,
      "loss": 0.7599,
      "step": 4160
    },
    {
      "epoch": 9.652777777777779,
      "grad_norm": 1.9494432210922241,
      "learning_rate": 1.931438127090301e-05,
      "loss": 0.5844,
      "step": 4170
    },
    {
      "epoch": 9.675925925925926,
      "grad_norm": 2.652921199798584,
      "learning_rate": 1.923076923076923e-05,
      "loss": 0.8444,
      "step": 4180
    },
    {
      "epoch": 9.699074074074074,
      "grad_norm": 2.4924814701080322,
      "learning_rate": 1.9147157190635452e-05,
      "loss": 0.6474,
      "step": 4190
    },
    {
      "epoch": 9.722222222222221,
      "grad_norm": 2.3746612071990967,
      "learning_rate": 1.9063545150501676e-05,
      "loss": 0.6155,
      "step": 4200
    },
    {
      "epoch": 9.74537037037037,
      "grad_norm": 3.357536554336548,
      "learning_rate": 1.8979933110367896e-05,
      "loss": 0.6465,
      "step": 4210
    },
    {
      "epoch": 9.768518518518519,
      "grad_norm": 3.127675771713257,
      "learning_rate": 1.8896321070234117e-05,
      "loss": 0.586,
      "step": 4220
    },
    {
      "epoch": 9.791666666666666,
      "grad_norm": 2.124647617340088,
      "learning_rate": 1.8812709030100337e-05,
      "loss": 0.7427,
      "step": 4230
    },
    {
      "epoch": 9.814814814814815,
      "grad_norm": 1.881104588508606,
      "learning_rate": 1.8729096989966554e-05,
      "loss": 0.6841,
      "step": 4240
    },
    {
      "epoch": 9.837962962962964,
      "grad_norm": 3.9461543560028076,
      "learning_rate": 1.8645484949832775e-05,
      "loss": 0.6905,
      "step": 4250
    },
    {
      "epoch": 9.86111111111111,
      "grad_norm": 3.2468643188476562,
      "learning_rate": 1.8561872909698995e-05,
      "loss": 0.7733,
      "step": 4260
    },
    {
      "epoch": 9.88425925925926,
      "grad_norm": 3.0709171295166016,
      "learning_rate": 1.8478260869565216e-05,
      "loss": 0.5697,
      "step": 4270
    },
    {
      "epoch": 9.907407407407408,
      "grad_norm": 2.2762460708618164,
      "learning_rate": 1.8394648829431436e-05,
      "loss": 0.6515,
      "step": 4280
    },
    {
      "epoch": 9.930555555555555,
      "grad_norm": 2.4506874084472656,
      "learning_rate": 1.831103678929766e-05,
      "loss": 0.6046,
      "step": 4290
    },
    {
      "epoch": 9.953703703703704,
      "grad_norm": 2.7228336334228516,
      "learning_rate": 1.822742474916388e-05,
      "loss": 0.7474,
      "step": 4300
    },
    {
      "epoch": 9.976851851851851,
      "grad_norm": 2.901038408279419,
      "learning_rate": 1.81438127090301e-05,
      "loss": 0.7459,
      "step": 4310
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.7356576919555664,
      "learning_rate": 1.806020066889632e-05,
      "loss": 0.6116,
      "step": 4320
    },
    {
      "epoch": 10.023148148148149,
      "grad_norm": 2.587360382080078,
      "learning_rate": 1.7976588628762542e-05,
      "loss": 0.6452,
      "step": 4330
    },
    {
      "epoch": 10.046296296296296,
      "grad_norm": 1.813063383102417,
      "learning_rate": 1.7892976588628763e-05,
      "loss": 0.5528,
      "step": 4340
    },
    {
      "epoch": 10.069444444444445,
      "grad_norm": 1.598973274230957,
      "learning_rate": 1.7809364548494983e-05,
      "loss": 0.5835,
      "step": 4350
    },
    {
      "epoch": 10.092592592592593,
      "grad_norm": 2.857927083969116,
      "learning_rate": 1.7725752508361204e-05,
      "loss": 0.6219,
      "step": 4360
    },
    {
      "epoch": 10.11574074074074,
      "grad_norm": 3.028810977935791,
      "learning_rate": 1.7642140468227424e-05,
      "loss": 0.5455,
      "step": 4370
    },
    {
      "epoch": 10.13888888888889,
      "grad_norm": 2.3225409984588623,
      "learning_rate": 1.7558528428093644e-05,
      "loss": 0.7085,
      "step": 4380
    },
    {
      "epoch": 10.162037037037036,
      "grad_norm": 1.9427739381790161,
      "learning_rate": 1.7474916387959865e-05,
      "loss": 0.6233,
      "step": 4390
    },
    {
      "epoch": 10.185185185185185,
      "grad_norm": 2.943816900253296,
      "learning_rate": 1.739130434782609e-05,
      "loss": 0.6802,
      "step": 4400
    },
    {
      "epoch": 10.208333333333334,
      "grad_norm": 2.62457537651062,
      "learning_rate": 1.730769230769231e-05,
      "loss": 0.7143,
      "step": 4410
    },
    {
      "epoch": 10.231481481481481,
      "grad_norm": 2.2381856441497803,
      "learning_rate": 1.722408026755853e-05,
      "loss": 0.5959,
      "step": 4420
    },
    {
      "epoch": 10.25462962962963,
      "grad_norm": 3.339818239212036,
      "learning_rate": 1.714046822742475e-05,
      "loss": 0.6914,
      "step": 4430
    },
    {
      "epoch": 10.277777777777779,
      "grad_norm": 1.5480589866638184,
      "learning_rate": 1.705685618729097e-05,
      "loss": 0.55,
      "step": 4440
    },
    {
      "epoch": 10.300925925925926,
      "grad_norm": 2.1415960788726807,
      "learning_rate": 1.697324414715719e-05,
      "loss": 0.6015,
      "step": 4450
    },
    {
      "epoch": 10.324074074074074,
      "grad_norm": 2.3992526531219482,
      "learning_rate": 1.6889632107023412e-05,
      "loss": 0.7036,
      "step": 4460
    },
    {
      "epoch": 10.347222222222221,
      "grad_norm": 2.5495548248291016,
      "learning_rate": 1.6806020066889632e-05,
      "loss": 0.7032,
      "step": 4470
    },
    {
      "epoch": 10.37037037037037,
      "grad_norm": 1.7585498094558716,
      "learning_rate": 1.6722408026755853e-05,
      "loss": 0.5348,
      "step": 4480
    },
    {
      "epoch": 10.393518518518519,
      "grad_norm": 2.043715238571167,
      "learning_rate": 1.6638795986622073e-05,
      "loss": 0.7447,
      "step": 4490
    },
    {
      "epoch": 10.416666666666666,
      "grad_norm": 3.3318610191345215,
      "learning_rate": 1.6555183946488294e-05,
      "loss": 0.7186,
      "step": 4500
    },
    {
      "epoch": 10.439814814814815,
      "grad_norm": 2.8536386489868164,
      "learning_rate": 1.6471571906354518e-05,
      "loss": 0.5841,
      "step": 4510
    },
    {
      "epoch": 10.462962962962964,
      "grad_norm": 2.3186464309692383,
      "learning_rate": 1.6387959866220738e-05,
      "loss": 0.5244,
      "step": 4520
    },
    {
      "epoch": 10.48611111111111,
      "grad_norm": 2.6392152309417725,
      "learning_rate": 1.630434782608696e-05,
      "loss": 0.6325,
      "step": 4530
    },
    {
      "epoch": 10.50925925925926,
      "grad_norm": 3.1082961559295654,
      "learning_rate": 1.622073578595318e-05,
      "loss": 0.678,
      "step": 4540
    },
    {
      "epoch": 10.532407407407408,
      "grad_norm": 2.349921226501465,
      "learning_rate": 1.61371237458194e-05,
      "loss": 0.6878,
      "step": 4550
    },
    {
      "epoch": 10.555555555555555,
      "grad_norm": 2.0006723403930664,
      "learning_rate": 1.605351170568562e-05,
      "loss": 0.5388,
      "step": 4560
    },
    {
      "epoch": 10.578703703703704,
      "grad_norm": 2.0211079120635986,
      "learning_rate": 1.596989966555184e-05,
      "loss": 0.6726,
      "step": 4570
    },
    {
      "epoch": 10.601851851851851,
      "grad_norm": 1.3574097156524658,
      "learning_rate": 1.588628762541806e-05,
      "loss": 0.6418,
      "step": 4580
    },
    {
      "epoch": 10.625,
      "grad_norm": 2.0365045070648193,
      "learning_rate": 1.580267558528428e-05,
      "loss": 0.6097,
      "step": 4590
    },
    {
      "epoch": 10.648148148148149,
      "grad_norm": 3.249819278717041,
      "learning_rate": 1.5719063545150502e-05,
      "loss": 0.7997,
      "step": 4600
    },
    {
      "epoch": 10.671296296296296,
      "grad_norm": 3.07437801361084,
      "learning_rate": 1.5635451505016722e-05,
      "loss": 0.767,
      "step": 4610
    },
    {
      "epoch": 10.694444444444445,
      "grad_norm": 3.1449005603790283,
      "learning_rate": 1.5551839464882946e-05,
      "loss": 0.63,
      "step": 4620
    },
    {
      "epoch": 10.717592592592592,
      "grad_norm": 2.316362142562866,
      "learning_rate": 1.5468227424749167e-05,
      "loss": 0.568,
      "step": 4630
    },
    {
      "epoch": 10.74074074074074,
      "grad_norm": 2.797579288482666,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.5678,
      "step": 4640
    },
    {
      "epoch": 10.76388888888889,
      "grad_norm": 2.94222354888916,
      "learning_rate": 1.5301003344481608e-05,
      "loss": 0.719,
      "step": 4650
    },
    {
      "epoch": 10.787037037037036,
      "grad_norm": 2.3123252391815186,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 0.5908,
      "step": 4660
    },
    {
      "epoch": 10.810185185185185,
      "grad_norm": 2.299656867980957,
      "learning_rate": 1.5133779264214049e-05,
      "loss": 0.6436,
      "step": 4670
    },
    {
      "epoch": 10.833333333333334,
      "grad_norm": 3.0586001873016357,
      "learning_rate": 1.5050167224080269e-05,
      "loss": 0.7375,
      "step": 4680
    },
    {
      "epoch": 10.856481481481481,
      "grad_norm": 2.2848598957061768,
      "learning_rate": 1.496655518394649e-05,
      "loss": 0.5851,
      "step": 4690
    },
    {
      "epoch": 10.87962962962963,
      "grad_norm": 2.084712028503418,
      "learning_rate": 1.4882943143812712e-05,
      "loss": 0.7713,
      "step": 4700
    },
    {
      "epoch": 10.902777777777779,
      "grad_norm": 2.1292107105255127,
      "learning_rate": 1.4799331103678932e-05,
      "loss": 0.7258,
      "step": 4710
    },
    {
      "epoch": 10.925925925925926,
      "grad_norm": 2.360612392425537,
      "learning_rate": 1.4715719063545153e-05,
      "loss": 0.6487,
      "step": 4720
    },
    {
      "epoch": 10.949074074074074,
      "grad_norm": 2.3294098377227783,
      "learning_rate": 1.4632107023411373e-05,
      "loss": 0.6771,
      "step": 4730
    },
    {
      "epoch": 10.972222222222221,
      "grad_norm": 2.2351858615875244,
      "learning_rate": 1.4548494983277592e-05,
      "loss": 0.6027,
      "step": 4740
    },
    {
      "epoch": 10.99537037037037,
      "grad_norm": 2.776430368423462,
      "learning_rate": 1.4464882943143812e-05,
      "loss": 0.672,
      "step": 4750
    },
    {
      "epoch": 11.018518518518519,
      "grad_norm": 1.4219157695770264,
      "learning_rate": 1.4381270903010033e-05,
      "loss": 0.7757,
      "step": 4760
    },
    {
      "epoch": 11.041666666666666,
      "grad_norm": 2.4369051456451416,
      "learning_rate": 1.4297658862876253e-05,
      "loss": 0.5117,
      "step": 4770
    },
    {
      "epoch": 11.064814814814815,
      "grad_norm": 2.32993745803833,
      "learning_rate": 1.4214046822742474e-05,
      "loss": 0.7283,
      "step": 4780
    },
    {
      "epoch": 11.087962962962964,
      "grad_norm": 2.5101282596588135,
      "learning_rate": 1.4130434782608694e-05,
      "loss": 0.678,
      "step": 4790
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 1.9299620389938354,
      "learning_rate": 1.4046822742474917e-05,
      "loss": 0.6732,
      "step": 4800
    },
    {
      "epoch": 11.13425925925926,
      "grad_norm": 2.0033762454986572,
      "learning_rate": 1.3963210702341137e-05,
      "loss": 0.7914,
      "step": 4810
    },
    {
      "epoch": 11.157407407407407,
      "grad_norm": 1.674472451210022,
      "learning_rate": 1.3879598662207358e-05,
      "loss": 0.5009,
      "step": 4820
    },
    {
      "epoch": 11.180555555555555,
      "grad_norm": 2.773608922958374,
      "learning_rate": 1.3795986622073578e-05,
      "loss": 0.5411,
      "step": 4830
    },
    {
      "epoch": 11.203703703703704,
      "grad_norm": 1.899234414100647,
      "learning_rate": 1.3712374581939799e-05,
      "loss": 0.5667,
      "step": 4840
    },
    {
      "epoch": 11.226851851851851,
      "grad_norm": 2.5380210876464844,
      "learning_rate": 1.362876254180602e-05,
      "loss": 0.6839,
      "step": 4850
    },
    {
      "epoch": 11.25,
      "grad_norm": 3.3550541400909424,
      "learning_rate": 1.3545150501672241e-05,
      "loss": 0.7266,
      "step": 4860
    },
    {
      "epoch": 11.273148148148149,
      "grad_norm": 2.103942394256592,
      "learning_rate": 1.3461538461538462e-05,
      "loss": 0.6358,
      "step": 4870
    },
    {
      "epoch": 11.296296296296296,
      "grad_norm": 2.8265469074249268,
      "learning_rate": 1.3377926421404682e-05,
      "loss": 0.6037,
      "step": 4880
    },
    {
      "epoch": 11.319444444444445,
      "grad_norm": 2.481869697570801,
      "learning_rate": 1.3294314381270903e-05,
      "loss": 0.5605,
      "step": 4890
    },
    {
      "epoch": 11.342592592592593,
      "grad_norm": 2.899794816970825,
      "learning_rate": 1.3210702341137123e-05,
      "loss": 0.5147,
      "step": 4900
    },
    {
      "epoch": 11.36574074074074,
      "grad_norm": 1.4600017070770264,
      "learning_rate": 1.3127090301003345e-05,
      "loss": 0.6661,
      "step": 4910
    },
    {
      "epoch": 11.38888888888889,
      "grad_norm": 1.94858717918396,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 0.6138,
      "step": 4920
    },
    {
      "epoch": 11.412037037037036,
      "grad_norm": 1.8368476629257202,
      "learning_rate": 1.2959866220735786e-05,
      "loss": 0.6168,
      "step": 4930
    },
    {
      "epoch": 11.435185185185185,
      "grad_norm": 2.6686317920684814,
      "learning_rate": 1.2876254180602007e-05,
      "loss": 0.6472,
      "step": 4940
    },
    {
      "epoch": 11.458333333333334,
      "grad_norm": 2.475299596786499,
      "learning_rate": 1.2792642140468227e-05,
      "loss": 0.7277,
      "step": 4950
    },
    {
      "epoch": 11.481481481481481,
      "grad_norm": 3.1313018798828125,
      "learning_rate": 1.270903010033445e-05,
      "loss": 0.7489,
      "step": 4960
    },
    {
      "epoch": 11.50462962962963,
      "grad_norm": 3.886861562728882,
      "learning_rate": 1.262541806020067e-05,
      "loss": 0.6968,
      "step": 4970
    },
    {
      "epoch": 11.527777777777779,
      "grad_norm": 1.9413038492202759,
      "learning_rate": 1.254180602006689e-05,
      "loss": 0.5388,
      "step": 4980
    },
    {
      "epoch": 11.550925925925926,
      "grad_norm": 1.9654362201690674,
      "learning_rate": 1.245819397993311e-05,
      "loss": 0.6709,
      "step": 4990
    },
    {
      "epoch": 11.574074074074074,
      "grad_norm": 1.727270245552063,
      "learning_rate": 1.2374581939799331e-05,
      "loss": 0.5754,
      "step": 5000
    },
    {
      "epoch": 11.597222222222221,
      "grad_norm": 2.786268711090088,
      "learning_rate": 1.2290969899665552e-05,
      "loss": 0.6831,
      "step": 5010
    },
    {
      "epoch": 11.62037037037037,
      "grad_norm": 2.009031057357788,
      "learning_rate": 1.2207357859531774e-05,
      "loss": 0.4907,
      "step": 5020
    },
    {
      "epoch": 11.643518518518519,
      "grad_norm": 3.310798168182373,
      "learning_rate": 1.2123745819397994e-05,
      "loss": 0.8194,
      "step": 5030
    },
    {
      "epoch": 11.666666666666666,
      "grad_norm": 2.3345930576324463,
      "learning_rate": 1.2040133779264215e-05,
      "loss": 0.5457,
      "step": 5040
    },
    {
      "epoch": 11.689814814814815,
      "grad_norm": 2.269826889038086,
      "learning_rate": 1.1956521739130435e-05,
      "loss": 0.7215,
      "step": 5050
    },
    {
      "epoch": 11.712962962962964,
      "grad_norm": 2.3369359970092773,
      "learning_rate": 1.1872909698996656e-05,
      "loss": 0.5425,
      "step": 5060
    },
    {
      "epoch": 11.73611111111111,
      "grad_norm": 2.2009894847869873,
      "learning_rate": 1.1789297658862878e-05,
      "loss": 0.59,
      "step": 5070
    },
    {
      "epoch": 11.75925925925926,
      "grad_norm": 3.1787164211273193,
      "learning_rate": 1.1705685618729099e-05,
      "loss": 0.6395,
      "step": 5080
    },
    {
      "epoch": 11.782407407407408,
      "grad_norm": 2.73893141746521,
      "learning_rate": 1.1622073578595319e-05,
      "loss": 0.7004,
      "step": 5090
    },
    {
      "epoch": 11.805555555555555,
      "grad_norm": 2.9378669261932373,
      "learning_rate": 1.153846153846154e-05,
      "loss": 0.5487,
      "step": 5100
    },
    {
      "epoch": 11.828703703703704,
      "grad_norm": 1.8557538986206055,
      "learning_rate": 1.1454849498327758e-05,
      "loss": 0.7887,
      "step": 5110
    },
    {
      "epoch": 11.851851851851851,
      "grad_norm": 2.364748239517212,
      "learning_rate": 1.137123745819398e-05,
      "loss": 0.5092,
      "step": 5120
    },
    {
      "epoch": 11.875,
      "grad_norm": 1.6686104536056519,
      "learning_rate": 1.1287625418060201e-05,
      "loss": 0.6114,
      "step": 5130
    },
    {
      "epoch": 11.898148148148149,
      "grad_norm": 2.549027442932129,
      "learning_rate": 1.1204013377926421e-05,
      "loss": 0.5874,
      "step": 5140
    },
    {
      "epoch": 11.921296296296296,
      "grad_norm": 3.2014882564544678,
      "learning_rate": 1.1120401337792642e-05,
      "loss": 0.4752,
      "step": 5150
    },
    {
      "epoch": 11.944444444444445,
      "grad_norm": 2.5295321941375732,
      "learning_rate": 1.1036789297658862e-05,
      "loss": 0.653,
      "step": 5160
    },
    {
      "epoch": 11.967592592592592,
      "grad_norm": 2.0633487701416016,
      "learning_rate": 1.0953177257525085e-05,
      "loss": 0.6221,
      "step": 5170
    },
    {
      "epoch": 11.99074074074074,
      "grad_norm": 2.25411319732666,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 0.5725,
      "step": 5180
    },
    {
      "epoch": 12.01388888888889,
      "grad_norm": 1.3471968173980713,
      "learning_rate": 1.0785953177257526e-05,
      "loss": 0.5232,
      "step": 5190
    },
    {
      "epoch": 12.037037037037036,
      "grad_norm": 2.1959571838378906,
      "learning_rate": 1.0702341137123746e-05,
      "loss": 0.5538,
      "step": 5200
    },
    {
      "epoch": 12.060185185185185,
      "grad_norm": 1.623983383178711,
      "learning_rate": 1.0618729096989967e-05,
      "loss": 0.6517,
      "step": 5210
    },
    {
      "epoch": 12.083333333333334,
      "grad_norm": 2.8135993480682373,
      "learning_rate": 1.0535117056856187e-05,
      "loss": 0.6142,
      "step": 5220
    },
    {
      "epoch": 12.106481481481481,
      "grad_norm": 3.307724952697754,
      "learning_rate": 1.045150501672241e-05,
      "loss": 0.6669,
      "step": 5230
    },
    {
      "epoch": 12.12962962962963,
      "grad_norm": 3.6155099868774414,
      "learning_rate": 1.036789297658863e-05,
      "loss": 0.6913,
      "step": 5240
    },
    {
      "epoch": 12.152777777777779,
      "grad_norm": 3.5730185508728027,
      "learning_rate": 1.028428093645485e-05,
      "loss": 0.5786,
      "step": 5250
    },
    {
      "epoch": 12.175925925925926,
      "grad_norm": 3.0062923431396484,
      "learning_rate": 1.020066889632107e-05,
      "loss": 0.6497,
      "step": 5260
    },
    {
      "epoch": 12.199074074074074,
      "grad_norm": 2.184108257293701,
      "learning_rate": 1.0117056856187291e-05,
      "loss": 0.684,
      "step": 5270
    },
    {
      "epoch": 12.222222222222221,
      "grad_norm": 2.9225640296936035,
      "learning_rate": 1.0033444816053512e-05,
      "loss": 0.5027,
      "step": 5280
    },
    {
      "epoch": 12.24537037037037,
      "grad_norm": 2.8726439476013184,
      "learning_rate": 9.949832775919734e-06,
      "loss": 0.5852,
      "step": 5290
    },
    {
      "epoch": 12.268518518518519,
      "grad_norm": 3.513198137283325,
      "learning_rate": 9.866220735785954e-06,
      "loss": 0.5613,
      "step": 5300
    },
    {
      "epoch": 12.291666666666666,
      "grad_norm": 2.361452102661133,
      "learning_rate": 9.782608695652175e-06,
      "loss": 0.5932,
      "step": 5310
    },
    {
      "epoch": 12.314814814814815,
      "grad_norm": 2.929335832595825,
      "learning_rate": 9.698996655518395e-06,
      "loss": 0.7384,
      "step": 5320
    },
    {
      "epoch": 12.337962962962964,
      "grad_norm": 2.4767963886260986,
      "learning_rate": 9.615384615384616e-06,
      "loss": 0.6536,
      "step": 5330
    },
    {
      "epoch": 12.36111111111111,
      "grad_norm": 3.5469322204589844,
      "learning_rate": 9.531772575250838e-06,
      "loss": 0.7396,
      "step": 5340
    },
    {
      "epoch": 12.38425925925926,
      "grad_norm": 2.294567584991455,
      "learning_rate": 9.448160535117058e-06,
      "loss": 0.6826,
      "step": 5350
    },
    {
      "epoch": 12.407407407407407,
      "grad_norm": 1.9463354349136353,
      "learning_rate": 9.364548494983277e-06,
      "loss": 0.5145,
      "step": 5360
    },
    {
      "epoch": 12.430555555555555,
      "grad_norm": 2.9607858657836914,
      "learning_rate": 9.280936454849498e-06,
      "loss": 0.6055,
      "step": 5370
    },
    {
      "epoch": 12.453703703703704,
      "grad_norm": 1.9591073989868164,
      "learning_rate": 9.197324414715718e-06,
      "loss": 0.5862,
      "step": 5380
    },
    {
      "epoch": 12.476851851851851,
      "grad_norm": 3.225468397140503,
      "learning_rate": 9.11371237458194e-06,
      "loss": 0.6237,
      "step": 5390
    },
    {
      "epoch": 12.5,
      "grad_norm": 2.9954042434692383,
      "learning_rate": 9.03010033444816e-06,
      "loss": 0.5569,
      "step": 5400
    },
    {
      "epoch": 12.523148148148149,
      "grad_norm": 2.447749137878418,
      "learning_rate": 8.946488294314381e-06,
      "loss": 0.5294,
      "step": 5410
    },
    {
      "epoch": 12.546296296296296,
      "grad_norm": 2.910541534423828,
      "learning_rate": 8.862876254180602e-06,
      "loss": 0.6643,
      "step": 5420
    },
    {
      "epoch": 12.569444444444445,
      "grad_norm": 1.5126785039901733,
      "learning_rate": 8.779264214046822e-06,
      "loss": 0.6625,
      "step": 5430
    },
    {
      "epoch": 12.592592592592592,
      "grad_norm": 3.5122485160827637,
      "learning_rate": 8.695652173913044e-06,
      "loss": 0.6455,
      "step": 5440
    },
    {
      "epoch": 12.61574074074074,
      "grad_norm": 3.2254974842071533,
      "learning_rate": 8.612040133779265e-06,
      "loss": 0.7701,
      "step": 5450
    },
    {
      "epoch": 12.63888888888889,
      "grad_norm": 3.084831476211548,
      "learning_rate": 8.528428093645485e-06,
      "loss": 0.7154,
      "step": 5460
    },
    {
      "epoch": 12.662037037037036,
      "grad_norm": 2.136871337890625,
      "learning_rate": 8.444816053511706e-06,
      "loss": 0.5835,
      "step": 5470
    },
    {
      "epoch": 12.685185185185185,
      "grad_norm": 2.781062364578247,
      "learning_rate": 8.361204013377926e-06,
      "loss": 0.5129,
      "step": 5480
    },
    {
      "epoch": 12.708333333333334,
      "grad_norm": 2.374913215637207,
      "learning_rate": 8.277591973244147e-06,
      "loss": 0.5625,
      "step": 5490
    },
    {
      "epoch": 12.731481481481481,
      "grad_norm": 2.6105809211730957,
      "learning_rate": 8.193979933110369e-06,
      "loss": 0.6202,
      "step": 5500
    },
    {
      "epoch": 12.75462962962963,
      "grad_norm": 1.994558572769165,
      "learning_rate": 8.11036789297659e-06,
      "loss": 0.5702,
      "step": 5510
    },
    {
      "epoch": 12.777777777777779,
      "grad_norm": 2.482257127761841,
      "learning_rate": 8.02675585284281e-06,
      "loss": 0.5951,
      "step": 5520
    },
    {
      "epoch": 12.800925925925926,
      "grad_norm": 2.8047780990600586,
      "learning_rate": 7.94314381270903e-06,
      "loss": 0.5808,
      "step": 5530
    },
    {
      "epoch": 12.824074074074074,
      "grad_norm": 2.485567569732666,
      "learning_rate": 7.859531772575251e-06,
      "loss": 0.4891,
      "step": 5540
    },
    {
      "epoch": 12.847222222222221,
      "grad_norm": 2.790302038192749,
      "learning_rate": 7.775919732441473e-06,
      "loss": 0.4626,
      "step": 5550
    },
    {
      "epoch": 12.87037037037037,
      "grad_norm": 2.7487711906433105,
      "learning_rate": 7.692307692307694e-06,
      "loss": 0.6618,
      "step": 5560
    },
    {
      "epoch": 12.893518518518519,
      "grad_norm": 2.1342039108276367,
      "learning_rate": 7.608695652173914e-06,
      "loss": 0.7496,
      "step": 5570
    },
    {
      "epoch": 12.916666666666666,
      "grad_norm": 1.9756665229797363,
      "learning_rate": 7.5250836120401346e-06,
      "loss": 0.637,
      "step": 5580
    },
    {
      "epoch": 12.939814814814815,
      "grad_norm": 1.8046023845672607,
      "learning_rate": 7.441471571906356e-06,
      "loss": 0.572,
      "step": 5590
    },
    {
      "epoch": 12.962962962962964,
      "grad_norm": 2.2958521842956543,
      "learning_rate": 7.357859531772576e-06,
      "loss": 0.5819,
      "step": 5600
    },
    {
      "epoch": 12.98611111111111,
      "grad_norm": 2.979592800140381,
      "learning_rate": 7.274247491638796e-06,
      "loss": 0.6215,
      "step": 5610
    },
    {
      "epoch": 13.00925925925926,
      "grad_norm": 1.5534192323684692,
      "learning_rate": 7.1906354515050165e-06,
      "loss": 0.5246,
      "step": 5620
    },
    {
      "epoch": 13.032407407407407,
      "grad_norm": 2.5936546325683594,
      "learning_rate": 7.107023411371237e-06,
      "loss": 0.6203,
      "step": 5630
    },
    {
      "epoch": 13.055555555555555,
      "grad_norm": 2.8475823402404785,
      "learning_rate": 7.023411371237458e-06,
      "loss": 0.6984,
      "step": 5640
    },
    {
      "epoch": 13.078703703703704,
      "grad_norm": 3.3090527057647705,
      "learning_rate": 6.939799331103679e-06,
      "loss": 0.8394,
      "step": 5650
    },
    {
      "epoch": 13.101851851851851,
      "grad_norm": 2.7550501823425293,
      "learning_rate": 6.856187290969899e-06,
      "loss": 0.5923,
      "step": 5660
    },
    {
      "epoch": 13.125,
      "grad_norm": 2.5943379402160645,
      "learning_rate": 6.772575250836121e-06,
      "loss": 0.513,
      "step": 5670
    },
    {
      "epoch": 13.148148148148149,
      "grad_norm": 2.7742788791656494,
      "learning_rate": 6.688963210702341e-06,
      "loss": 0.5814,
      "step": 5680
    },
    {
      "epoch": 13.171296296296296,
      "grad_norm": 1.84011971950531,
      "learning_rate": 6.6053511705685616e-06,
      "loss": 0.5899,
      "step": 5690
    },
    {
      "epoch": 13.194444444444445,
      "grad_norm": 1.9177647829055786,
      "learning_rate": 6.521739130434783e-06,
      "loss": 0.505,
      "step": 5700
    },
    {
      "epoch": 13.217592592592593,
      "grad_norm": 1.3345144987106323,
      "learning_rate": 6.438127090301003e-06,
      "loss": 0.5342,
      "step": 5710
    },
    {
      "epoch": 13.24074074074074,
      "grad_norm": 3.3163185119628906,
      "learning_rate": 6.354515050167225e-06,
      "loss": 0.5731,
      "step": 5720
    },
    {
      "epoch": 13.26388888888889,
      "grad_norm": 2.6255509853363037,
      "learning_rate": 6.270903010033445e-06,
      "loss": 0.5322,
      "step": 5730
    },
    {
      "epoch": 13.287037037037036,
      "grad_norm": 2.985032320022583,
      "learning_rate": 6.187290969899666e-06,
      "loss": 0.6821,
      "step": 5740
    },
    {
      "epoch": 13.310185185185185,
      "grad_norm": 3.240601062774658,
      "learning_rate": 6.103678929765887e-06,
      "loss": 0.5186,
      "step": 5750
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 2.625741720199585,
      "learning_rate": 6.0200668896321075e-06,
      "loss": 0.573,
      "step": 5760
    },
    {
      "epoch": 13.356481481481481,
      "grad_norm": 2.148689031600952,
      "learning_rate": 5.936454849498328e-06,
      "loss": 0.501,
      "step": 5770
    },
    {
      "epoch": 13.37962962962963,
      "grad_norm": 2.0208346843719482,
      "learning_rate": 5.852842809364549e-06,
      "loss": 0.5511,
      "step": 5780
    },
    {
      "epoch": 13.402777777777779,
      "grad_norm": 1.459582805633545,
      "learning_rate": 5.76923076923077e-06,
      "loss": 0.6116,
      "step": 5790
    },
    {
      "epoch": 13.425925925925926,
      "grad_norm": 2.6043894290924072,
      "learning_rate": 5.68561872909699e-06,
      "loss": 0.5813,
      "step": 5800
    },
    {
      "epoch": 13.449074074074074,
      "grad_norm": 3.4518771171569824,
      "learning_rate": 5.602006688963211e-06,
      "loss": 0.6708,
      "step": 5810
    },
    {
      "epoch": 13.472222222222221,
      "grad_norm": 2.739556312561035,
      "learning_rate": 5.518394648829431e-06,
      "loss": 0.5337,
      "step": 5820
    },
    {
      "epoch": 13.49537037037037,
      "grad_norm": 2.939065456390381,
      "learning_rate": 5.4347826086956525e-06,
      "loss": 0.6928,
      "step": 5830
    },
    {
      "epoch": 13.518518518518519,
      "grad_norm": 2.4591052532196045,
      "learning_rate": 5.351170568561873e-06,
      "loss": 0.5226,
      "step": 5840
    },
    {
      "epoch": 13.541666666666666,
      "grad_norm": 2.1778945922851562,
      "learning_rate": 5.2675585284280935e-06,
      "loss": 0.5525,
      "step": 5850
    },
    {
      "epoch": 13.564814814814815,
      "grad_norm": 3.477821111679077,
      "learning_rate": 5.183946488294315e-06,
      "loss": 0.6625,
      "step": 5860
    },
    {
      "epoch": 13.587962962962964,
      "grad_norm": 2.409802198410034,
      "learning_rate": 5.100334448160535e-06,
      "loss": 0.543,
      "step": 5870
    },
    {
      "epoch": 13.61111111111111,
      "grad_norm": 1.5086480379104614,
      "learning_rate": 5.016722408026756e-06,
      "loss": 0.6378,
      "step": 5880
    },
    {
      "epoch": 13.63425925925926,
      "grad_norm": 1.547042727470398,
      "learning_rate": 4.933110367892977e-06,
      "loss": 0.5448,
      "step": 5890
    },
    {
      "epoch": 13.657407407407408,
      "grad_norm": 1.8462026119232178,
      "learning_rate": 4.849498327759198e-06,
      "loss": 0.612,
      "step": 5900
    },
    {
      "epoch": 13.680555555555555,
      "grad_norm": 2.6166913509368896,
      "learning_rate": 4.765886287625419e-06,
      "loss": 0.6401,
      "step": 5910
    },
    {
      "epoch": 13.703703703703704,
      "grad_norm": 2.2434794902801514,
      "learning_rate": 4.682274247491639e-06,
      "loss": 0.6147,
      "step": 5920
    },
    {
      "epoch": 13.726851851851851,
      "grad_norm": 2.688795566558838,
      "learning_rate": 4.598662207357859e-06,
      "loss": 0.6226,
      "step": 5930
    },
    {
      "epoch": 13.75,
      "grad_norm": 1.5853441953659058,
      "learning_rate": 4.51505016722408e-06,
      "loss": 0.5009,
      "step": 5940
    },
    {
      "epoch": 13.773148148148149,
      "grad_norm": 2.6594953536987305,
      "learning_rate": 4.431438127090301e-06,
      "loss": 0.7734,
      "step": 5950
    },
    {
      "epoch": 13.796296296296296,
      "grad_norm": 1.6776716709136963,
      "learning_rate": 4.347826086956522e-06,
      "loss": 0.6334,
      "step": 5960
    },
    {
      "epoch": 13.819444444444445,
      "grad_norm": 2.697115898132324,
      "learning_rate": 4.264214046822743e-06,
      "loss": 0.6853,
      "step": 5970
    },
    {
      "epoch": 13.842592592592592,
      "grad_norm": 2.312268018722534,
      "learning_rate": 4.180602006688963e-06,
      "loss": 0.5456,
      "step": 5980
    },
    {
      "epoch": 13.86574074074074,
      "grad_norm": 2.215839147567749,
      "learning_rate": 4.0969899665551845e-06,
      "loss": 0.532,
      "step": 5990
    },
    {
      "epoch": 13.88888888888889,
      "grad_norm": 2.115535259246826,
      "learning_rate": 4.013377926421405e-06,
      "loss": 0.5996,
      "step": 6000
    },
    {
      "epoch": 13.912037037037036,
      "grad_norm": 2.1126930713653564,
      "learning_rate": 3.9297658862876255e-06,
      "loss": 0.5827,
      "step": 6010
    },
    {
      "epoch": 13.935185185185185,
      "grad_norm": 2.5490267276763916,
      "learning_rate": 3.846153846153847e-06,
      "loss": 0.5299,
      "step": 6020
    },
    {
      "epoch": 13.958333333333334,
      "grad_norm": 2.4806480407714844,
      "learning_rate": 3.7625418060200673e-06,
      "loss": 0.5892,
      "step": 6030
    },
    {
      "epoch": 13.981481481481481,
      "grad_norm": 2.400343656539917,
      "learning_rate": 3.678929765886288e-06,
      "loss": 0.7105,
      "step": 6040
    },
    {
      "epoch": 14.00462962962963,
      "grad_norm": 2.3434014320373535,
      "learning_rate": 3.5953177257525082e-06,
      "loss": 0.6332,
      "step": 6050
    },
    {
      "epoch": 14.027777777777779,
      "grad_norm": 2.7410390377044678,
      "learning_rate": 3.511705685618729e-06,
      "loss": 0.532,
      "step": 6060
    },
    {
      "epoch": 14.050925925925926,
      "grad_norm": 2.3081445693969727,
      "learning_rate": 3.4280936454849496e-06,
      "loss": 0.5185,
      "step": 6070
    },
    {
      "epoch": 14.074074074074074,
      "grad_norm": 2.1227245330810547,
      "learning_rate": 3.3444816053511705e-06,
      "loss": 0.6493,
      "step": 6080
    },
    {
      "epoch": 14.097222222222221,
      "grad_norm": 2.062492847442627,
      "learning_rate": 3.2608695652173914e-06,
      "loss": 0.661,
      "step": 6090
    },
    {
      "epoch": 14.12037037037037,
      "grad_norm": 2.552093029022217,
      "learning_rate": 3.1772575250836123e-06,
      "loss": 0.6537,
      "step": 6100
    },
    {
      "epoch": 14.143518518518519,
      "grad_norm": 2.224431037902832,
      "learning_rate": 3.093645484949833e-06,
      "loss": 0.5526,
      "step": 6110
    },
    {
      "epoch": 14.166666666666666,
      "grad_norm": 2.488478899002075,
      "learning_rate": 3.0100334448160537e-06,
      "loss": 0.6146,
      "step": 6120
    },
    {
      "epoch": 14.189814814814815,
      "grad_norm": 3.0201289653778076,
      "learning_rate": 2.9264214046822746e-06,
      "loss": 0.6708,
      "step": 6130
    },
    {
      "epoch": 14.212962962962964,
      "grad_norm": 3.696092367172241,
      "learning_rate": 2.842809364548495e-06,
      "loss": 0.6332,
      "step": 6140
    },
    {
      "epoch": 14.23611111111111,
      "grad_norm": 2.8484318256378174,
      "learning_rate": 2.7591973244147156e-06,
      "loss": 0.6125,
      "step": 6150
    },
    {
      "epoch": 14.25925925925926,
      "grad_norm": 2.8949642181396484,
      "learning_rate": 2.6755852842809365e-06,
      "loss": 0.6691,
      "step": 6160
    },
    {
      "epoch": 14.282407407407407,
      "grad_norm": 2.8285329341888428,
      "learning_rate": 2.5919732441471574e-06,
      "loss": 0.5082,
      "step": 6170
    },
    {
      "epoch": 14.305555555555555,
      "grad_norm": 2.0125575065612793,
      "learning_rate": 2.508361204013378e-06,
      "loss": 0.424,
      "step": 6180
    },
    {
      "epoch": 14.328703703703704,
      "grad_norm": 2.6180858612060547,
      "learning_rate": 2.424749163879599e-06,
      "loss": 0.6257,
      "step": 6190
    },
    {
      "epoch": 14.351851851851851,
      "grad_norm": 1.7979103326797485,
      "learning_rate": 2.3411371237458193e-06,
      "loss": 0.5673,
      "step": 6200
    },
    {
      "epoch": 14.375,
      "grad_norm": 1.9546152353286743,
      "learning_rate": 2.25752508361204e-06,
      "loss": 0.618,
      "step": 6210
    },
    {
      "epoch": 14.398148148148149,
      "grad_norm": 2.2886552810668945,
      "learning_rate": 2.173913043478261e-06,
      "loss": 0.5255,
      "step": 6220
    },
    {
      "epoch": 14.421296296296296,
      "grad_norm": 2.2837915420532227,
      "learning_rate": 2.0903010033444816e-06,
      "loss": 0.6895,
      "step": 6230
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 2.2340757846832275,
      "learning_rate": 2.0066889632107025e-06,
      "loss": 0.5277,
      "step": 6240
    },
    {
      "epoch": 14.467592592592593,
      "grad_norm": 1.6820274591445923,
      "learning_rate": 1.9230769230769234e-06,
      "loss": 0.6167,
      "step": 6250
    },
    {
      "epoch": 14.49074074074074,
      "grad_norm": 2.898801803588867,
      "learning_rate": 1.839464882943144e-06,
      "loss": 0.5169,
      "step": 6260
    },
    {
      "epoch": 14.51388888888889,
      "grad_norm": 2.3223211765289307,
      "learning_rate": 1.7558528428093646e-06,
      "loss": 0.5864,
      "step": 6270
    },
    {
      "epoch": 14.537037037037036,
      "grad_norm": 2.7522597312927246,
      "learning_rate": 1.6722408026755853e-06,
      "loss": 0.4293,
      "step": 6280
    },
    {
      "epoch": 14.560185185185185,
      "grad_norm": 2.0120432376861572,
      "learning_rate": 1.5886287625418062e-06,
      "loss": 0.656,
      "step": 6290
    },
    {
      "epoch": 14.583333333333334,
      "grad_norm": 2.7115426063537598,
      "learning_rate": 1.5050167224080269e-06,
      "loss": 0.5051,
      "step": 6300
    },
    {
      "epoch": 14.606481481481481,
      "grad_norm": 1.4628050327301025,
      "learning_rate": 1.4214046822742476e-06,
      "loss": 0.7063,
      "step": 6310
    },
    {
      "epoch": 14.62962962962963,
      "grad_norm": 2.7767887115478516,
      "learning_rate": 1.3377926421404683e-06,
      "loss": 0.5483,
      "step": 6320
    },
    {
      "epoch": 14.652777777777779,
      "grad_norm": 2.327054977416992,
      "learning_rate": 1.254180602006689e-06,
      "loss": 0.5874,
      "step": 6330
    },
    {
      "epoch": 14.675925925925926,
      "grad_norm": 3.3560264110565186,
      "learning_rate": 1.1705685618729096e-06,
      "loss": 0.6344,
      "step": 6340
    },
    {
      "epoch": 14.699074074074074,
      "grad_norm": 2.7573482990264893,
      "learning_rate": 1.0869565217391306e-06,
      "loss": 0.7144,
      "step": 6350
    },
    {
      "epoch": 14.722222222222221,
      "grad_norm": 2.9383089542388916,
      "learning_rate": 1.0033444816053512e-06,
      "loss": 0.5542,
      "step": 6360
    },
    {
      "epoch": 14.74537037037037,
      "grad_norm": 2.250298261642456,
      "learning_rate": 9.19732441471572e-07,
      "loss": 0.6085,
      "step": 6370
    },
    {
      "epoch": 14.768518518518519,
      "grad_norm": 1.9407700300216675,
      "learning_rate": 8.361204013377926e-07,
      "loss": 0.5689,
      "step": 6380
    },
    {
      "epoch": 14.791666666666666,
      "grad_norm": 2.271315097808838,
      "learning_rate": 7.525083612040134e-07,
      "loss": 0.4497,
      "step": 6390
    },
    {
      "epoch": 14.814814814814815,
      "grad_norm": 1.9129719734191895,
      "learning_rate": 6.688963210702341e-07,
      "loss": 0.6162,
      "step": 6400
    },
    {
      "epoch": 14.837962962962964,
      "grad_norm": 2.3424954414367676,
      "learning_rate": 5.852842809364548e-07,
      "loss": 0.5267,
      "step": 6410
    },
    {
      "epoch": 14.86111111111111,
      "grad_norm": 1.7246285676956177,
      "learning_rate": 5.016722408026756e-07,
      "loss": 0.5525,
      "step": 6420
    },
    {
      "epoch": 14.88425925925926,
      "grad_norm": 2.708348035812378,
      "learning_rate": 4.180602006688963e-07,
      "loss": 0.6354,
      "step": 6430
    },
    {
      "epoch": 14.907407407407408,
      "grad_norm": 1.8398531675338745,
      "learning_rate": 3.3444816053511706e-07,
      "loss": 0.563,
      "step": 6440
    },
    {
      "epoch": 14.930555555555555,
      "grad_norm": 2.5253937244415283,
      "learning_rate": 2.508361204013378e-07,
      "loss": 0.7672,
      "step": 6450
    },
    {
      "epoch": 14.953703703703704,
      "grad_norm": 2.1223151683807373,
      "learning_rate": 1.6722408026755853e-07,
      "loss": 0.4832,
      "step": 6460
    },
    {
      "epoch": 14.976851851851851,
      "grad_norm": 2.540499687194824,
      "learning_rate": 8.361204013377927e-08,
      "loss": 0.6096,
      "step": 6470
    },
    {
      "epoch": 15.0,
      "grad_norm": 3.0998003482818604,
      "learning_rate": 0.0,
      "loss": 0.6364,
      "step": 6480
    }
  ],
  "logging_steps": 10,
  "max_steps": 6480,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3386344734720000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
